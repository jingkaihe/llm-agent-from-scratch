{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an LLM Agent from Scratch\n",
    "\n",
    "It looks like magic when you witness AI agents autonomously viewing files, making edits, executing commands, and working around errors. However, under the hood, the core loop is surprisingly simple: it runs a continuous cycle that takes user input, generates tool calls and text output, receives feedback from tool execution, feeds the result back to the LLM, and repeats. The complexity lies not in any orchestration, but in the sophisticated reasoning capabilities of the frontier labs' large language models.\n",
    "\n",
    "To demystify this magic, we'll implement a simple CLI-based coding agent in Python that can read files, write content, edit text, and execute shell commands. Don't worry if you're new to AI agents - this tutorial is designed to be approachable and practical. You'll be amazed at how much you can accomplish with surprisingly little code.\n",
    "\n",
    "The diagram below illustrates the fundamental agentic loop in action:\n",
    "\n",
    " ```mermaid\n",
    " graph TD\n",
    "     Start[User Input] --> Loop[LLM Inference]\n",
    "\n",
    "     Loop -->|generates| Output[LLM Output]\n",
    "\n",
    "     Output --> EndsCheck{Ends?}\n",
    "\n",
    "     EndsCheck -->|Yes| Exit[Break/Exit Loop]\n",
    "\n",
    "     EndsCheck -->|No| ToolCheck{Tool Calls?}\n",
    "\n",
    "     ToolCheck -->|Yes| Tools[Execute Tool Calls]\n",
    "     Tools -->|tool results as new user input| Loop\n",
    "\n",
    "     ToolCheck -->|No| Loop\n",
    "\n",
    "     style EndsCheck fill:#faa,stroke:#333,stroke-width:2px\n",
    "     style ToolCheck fill:#f9f,stroke:#333,stroke-width:2px\n",
    "     style Tools fill:#bfb,stroke:#333,stroke-width:2px\n",
    "     style Exit fill:#ddd,stroke:#333,stroke-width:2px\n",
    " ```\n",
    "\n",
    "By the end of this tutorial, we'll have a Python script that's just over a handred lines of code - compact, powerful, and surprisingly capable of giving you the 'Oh look at that!' dopamine hits. I've structured this to be easy to follow along step by step, and I highly encourage you to try it out yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "You need an Anthropic API key for accessing to Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "if not api_key:\n",
    "    api_key = getpass.getpass(\"Enter your Anthropic API key: \")\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = api_key# Your agent code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Core Loop\n",
    "\n",
    "Let's start with the foundation. Every AI agent needs a chat loop that maintains context and handles the back-and-forth between user and AI. Here's our basic structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  exit\n"
     ]
    }
   ],
   "source": [
    "from anthropic import AsyncAnthropic\n",
    "import json, os, asyncio\n",
    "from datetime import datetime\n",
    "\n",
    "client = AsyncAnthropic()\n",
    "\n",
    "async def loop(system_prompt, messages, user_input):\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    msg = await client.messages.create(\n",
    "        max_tokens=2048,\n",
    "        thinking={\"type\": \"enabled\", \"budget_tokens\": 1024},\n",
    "        system=[{\"type\": \"text\", \"text\": system_prompt}],\n",
    "        messages=messages,\n",
    "        model=\"claude-sonnet-4-5\"\n",
    "    )\n",
    "\n",
    "    messages.append({\"role\": \"assistant\", \"content\": msg.content})\n",
    "\n",
    "    thinking_text = ' '.join(t.thinking for t in msg.content if t.type == 'thinking')\n",
    "    if thinking_text:\n",
    "        print(f\"ðŸ’­ {thinking_text}\")\n",
    "\n",
    "    agent_text = ' '.join(t.text for t in msg.content if t.type == 'text')\n",
    "    if agent_text:\n",
    "        print(f\"ðŸ¤– {agent_text}\")\n",
    "\n",
    "async def main():\n",
    "    messages = []\n",
    "    system_prompt = f\"\"\"Your name is HAL.\n",
    "You are an interactive CLI tool that helps with software engineering and production operations tasks.\n",
    "You are running on {str(os.uname())}, today is {datetime.now().strftime('%Y-%m-%d')}\n",
    "\"\"\"\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"> \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            break\n",
    "        await loop(system_prompt, messages, user_input)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script establishes the core conversation loop of our agent. The loop function handles the core conversation cycle: it adds user input to the message history, sends everything to Claude, processes the response, and displays it. The thinking parameter enables Claude's extended thinking and reasoning capabilities with a 1024-token budget, allowing it to work through complex problems step-by-step thought process. The main function creates an interactive REPL that maintains conversation context across exchanges.\n",
    "\n",
    "This gives us a working chat interface where we can have fascinating conversations with HAL, after all it embodies vast human knowledge from across the internet. However, HAL remains confined to the realm of text and imagesâ€”the only way for it to interact with the outer world is through you manually feeding it information such as code, logs, compiler errors, etc., which is not only inefficient but also potentially biased. To bridge this gap and give it the ability to act in the real world, we need to equip it with tools so that it can read files, run programs to get results and errors autonomously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Tool Capabilities\n",
    "\n",
    "As for large language models, the most straightforward way to feed them with external information is to provide them the ability to read text, therefore the first tool we'll implement is file reading. The function below shows file reading in its simplest form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiofiles\n",
    "from typing import Annotated\n",
    "\n",
    "async def read_file(filename: Annotated[str, \"The path to the file to read\"]):\n",
    "    \"\"\"Read the whole file\"\"\"\n",
    "    async with aiofiles.open(filename, 'r') as f:\n",
    "        return {\"success\": True, \"filename\": filename, \"output\": await f.read()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function works, but HAL doesn't know its existence, neither does it have access to the Python runtime. We need to bridge the gap between Python functions and jsonschemas that the model can understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tool System\n",
    "\n",
    "Here's where the magic happens. We'll create a decorator that automatically generates the necessary schemas and a toolbox to manage everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, get_type_hints, get_origin, get_args\n",
    "from inspect import Parameter, signature\n",
    "from pydantic import create_model, Field\n",
    "\n",
    "\n",
    "def tool(f):\n",
    "    def _process_parameter(name: str, param: Parameter, hints: dict) -> tuple:\n",
    "        \"\"\"Process a single parameter into a model field specification.\"\"\"\n",
    "        default = ... if param.default == Parameter.empty else param.default\n",
    "        hint = hints.get(name, param.annotation)\n",
    "\n",
    "        if get_origin(hint) == Annotated:\n",
    "            base_type, *metadata = get_args(hint)\n",
    "            description = next((m for m in metadata if isinstance(m, str)), None)\n",
    "            return (base_type, Field(default=default, description=description) if description else default)\n",
    "\n",
    "        return (hint, default)\n",
    "\n",
    "    hints = get_type_hints(f, include_extras=True)\n",
    "    model_fields = { name: _process_parameter(name, param, hints) for name, param in signature(f).parameters.items() }\n",
    "\n",
    "    m = create_model(f'{f.__name__} Input', **model_fields)\n",
    "    m.run = lambda self: f(**self.model_dump())\n",
    "\n",
    "    return {\n",
    "        \"name\": f.__name__,\n",
    "        \"description\": f.__doc__ or f\"Tool: {f.__name__}\",\n",
    "        \"model\": m\n",
    "    }\n",
    "\n",
    "class Toolbox:\n",
    "    def __init__(self, tools):\n",
    "        self.tools = tools\n",
    "\n",
    "    def schema(self):\n",
    "        return [{\n",
    "            \"name\": t[\"name\"],\n",
    "            \"description\": t[\"description\"],\n",
    "            \"input_schema\": t[\"model\"].model_json_schema()\n",
    "        } for t in self.tools]\n",
    "\n",
    "    async def run(self, name, input):\n",
    "        tool = next(t for t in self.tools if t[\"name\"] == name)\n",
    "        return await tool[\"model\"](**input).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At its core, this code creates a bridge between Python functions and AI models. The @tool decorator examines function's signature and automatically generates a JSON schema that describes what inputs the function expects - think of it as creating a user manual for the Python function that the AI can read. The Toolbox class acts like a registry, collecting all these tool schemas and providing a way to execute them when the AI requests it. When HAL wants to use a tool, it sends a structured output that matches the schema, and the toolbox validates the input and runs the corresponding Python function.\n",
    "\n",
    "**Also don't worry if you can't understand what this function does. Just use this as a utility**\n",
    "\n",
    "Now we can decorate our file reading function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "async def read_file(filename: Annotated[str, \"The path to the file to read\"]):\n",
    "    \"\"\"Read the whole file\"\"\"\n",
    "    async with aiofiles.open(filename, 'r') as f:\n",
    "        return {\"success\": True, \"filename\": filename, \"output\": await f.read()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once decorated, we can generate a JSON schema that describes the function to the AI model. Each schema at minimum contains the function's name, description, and parameter specifications. This schema acts as a contract when HAL wants to use a tool, it must structure its output according to this schema, making it the protocol between the HAL and the Python runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"name\": \"read_file\",\n",
      "    \"description\": \"Read the whole file\",\n",
      "    \"input_schema\": {\n",
      "      \"properties\": {\n",
      "        \"filename\": {\n",
      "          \"description\": \"The path to the file to read\",\n",
      "          \"title\": \"Filename\",\n",
      "          \"type\": \"string\"\n",
      "        }\n",
      "      },\n",
      "      \"required\": [\n",
      "        \"filename\"\n",
      "      ],\n",
      "      \"title\": \"read_file Input\",\n",
      "      \"type\": \"object\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "toolbox = Toolbox([read_file])\n",
    "print(json.dumps(toolbox.schema(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting Tools to the Loop\n",
    "Now we need to update our main loop to handle tool calls. When Claude wants to use a tool, it returns special tool_use blocks instead of just text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def loop(system_prompt, toolbox, messages, user_input):\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    while True:\n",
    "        msg = await client.messages.create(\n",
    "            max_tokens=2048,\n",
    "            thinking={\"type\": \"enabled\", \"budget_tokens\": 1024},\n",
    "            system=[{\"type\": \"text\", \"text\": system_prompt}],\n",
    "            messages=messages,\n",
    "            model=\"claude-sonnet-4-5\",\n",
    "            tools=toolbox.schema()\n",
    "        )\n",
    "\n",
    "        messages.append({\"role\": \"assistant\", \"content\": msg.content})\n",
    "\n",
    "        thinking_text = ' '.join(t.thinking for t in msg.content if t.type == 'thinking')\n",
    "        if thinking_text:\n",
    "            print(f\"ðŸ’­ {thinking_text}\")\n",
    "\n",
    "        agent_text = ' '.join(t.text for t in msg.content if t.type == 'text')\n",
    "        if agent_text:\n",
    "            print(f\"ðŸ¤– {agent_text}\")\n",
    "\n",
    "        tools = [t for t in msg.content if t.type == \"tool_use\"]\n",
    "        if not tools:\n",
    "            break\n",
    "\n",
    "        # Execute all tools and collect results\n",
    "        results = await asyncio.gather(*[\n",
    "            toolbox.run(t.name, t.input) for t in tools\n",
    "        ])\n",
    "\n",
    "        # Display results and send back to model\n",
    "        for t, r in zip(tools, results):\n",
    "            status = \"âœ…\" if r.get(\"success\") else \"âŒ\"\n",
    "            print(f\"{status} {t.name}:\")\n",
    "            print(json.dumps(r, indent=2))\n",
    "\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\n",
    "                \"type\": \"tool_result\",\n",
    "                \"tool_use_id\": t.id,\n",
    "                \"content\": json.dumps(r)\n",
    "            } for t, r in zip(tools, results)]\n",
    "        })\n",
    "\n",
    "async def run_agent(tools=[]):\n",
    "    toolbox = Toolbox(tools)\n",
    "    messages = []\n",
    "    system_prompt = f\"\"\"Your name is HAL.\n",
    "You are an interactive CLI tool that helps with software engineering and production operations tasks.\n",
    "You are running on {str(os.uname())}, today is {datetime.now().strftime('%Y-%m-%d')}\n",
    "\"\"\"\n",
    "    print(\"enter 'exit' to quit\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            user_input = input(\"> \")\n",
    "            if user_input.lower() == \"exit\":\n",
    "                print(\"ðŸ‘‹ Goodbye!\")\n",
    "                break\n",
    "            await loop(system_prompt, toolbox, messages, user_input)\n",
    "    except (KeyboardInterrupt, EOFError):\n",
    "        print(\"ðŸ‘‹ Goodbye!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter 'exit' to quit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‹ Goodbye!\n"
     ]
    }
   ],
   "source": [
    "await run_agent(tools=[read_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding More Tools\n",
    "With our infrastructure in place, adding more capabilities becomes straightforward. Let's implement three essential tools: file writing, editing, and shell execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "async def write_file(filename: str, content: str):\n",
    "    \"\"\"Write the file with the content. This is an overwrite\"\"\"\n",
    "    async with aiofiles.open(filename, 'w') as f:\n",
    "        await f.write(content)\n",
    "        return {\"success\": True, \"filename\": filename, \"output\": f\"wrote to {filename}\"}\n",
    "\n",
    "@tool\n",
    "async def edit_file(filename: str, old_text: Annotated[str, \"the text to replace\"], new_text: Annotated[str, \"the text to replace with\"]):\n",
    "    \"\"\"Edit the file with the new text. Note that the text to replace should only appear once in the file.\"\"\"\n",
    "    async with aiofiles.open(filename, 'r') as f:\n",
    "        content = await f.read()\n",
    "\n",
    "    if content.count(old_text) != 1:\n",
    "        return {\"success\": False, \"filename\": filename, \"output\": f\"old text appears {content.count(old_text)} times in the file\"}\n",
    "\n",
    "    async with aiofiles.open(filename, 'w') as f:\n",
    "        await f.write(content.replace(old_text, new_text))\n",
    "\n",
    "    return {\"success\": True, \"filename\": filename, \"output\": f\"edited {filename}\"}\n",
    "\n",
    "@tool\n",
    "async def shell(command: Annotated[str, \"command to execute\"], timeout: Annotated[int, \"timeout in seconds\"] = 30):\n",
    "    \"\"\"Execute a bash command\"\"\"\n",
    "    try:\n",
    "        p = await asyncio.create_subprocess_shell(\n",
    "            command,\n",
    "            stdout=asyncio.subprocess.PIPE,\n",
    "            stderr=asyncio.subprocess.STDOUT\n",
    "        )\n",
    "        stdout, _ = await asyncio.wait_for(p.communicate(), timeout)\n",
    "        return {\"success\": p.returncode == 0, \"command\": command, \"output\": stdout.decode()}\n",
    "    except asyncio.TimeoutError:\n",
    "        return {\"success\": False, \"command\": command, \"output\": \"Timeout\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together\n",
    "Finally, we wire everything up in our main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter 'exit' to quit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‹ Goodbye!\n"
     ]
    }
   ],
   "source": [
    "await run_agent(tools=[read_file, write_file, edit_file, shell])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What many people don't realise is that frontier models like Claude possess incredible system operations knowledge. Once you provide it with shell access and basic system information, even without extensive prompting, these models demonstrate expert-level operations capabilities. Claude Sonnet models in particular exhibit **persistent, relentless and infectiously optimistic** problem-solving behavior. When asked a question like \"how is python3 installed on this machine?\", they will systematically explore the filesystem, examine package managers, check environment variables, and execute diagnostic commands until they find a definitive answer.\n",
    "\n",
    "When commands fail or produce errors, the model treats these as valuable debugging signals, much like an experienced engineer would. It analyses error messages, adjusts its approach, and tries alternative strategies to work around the problem.\n",
    "\n",
    "While hallucination remains a concern (perhaps one of the most common criticisms of LLMs), the model's ability to execute commands creates a unreasonably effective feedback loop. When its assumptions prove incorrect, the resulting errors quickly ground the model back in reality, forcing it to revise its mental model and try new approaches based on actual system behavior rather than potentially outdated training data or hallucinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCP MCP MCP\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
