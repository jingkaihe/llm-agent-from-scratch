{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an LLM Agent from Scratch\n",
    "\n",
    "It looks like magic when you witness AI agents autonomously viewing files, making edits, executing commands, and working around errors. However, under the hood, the core loop is surprisingly simple: it runs a continuous cycle that takes user input, generates tool calls and text output, receives feedback from tool execution, feeds the result back to the LLM, and repeats. The complexity lies not in any orchestration, but in the sophisticated reasoning capabilities of the frontier labs' large language models.\n",
    "\n",
    "To demystify this magic, we'll implement a simple CLI-based coding agent in Python that can read files, write content, edit text, and execute shell commands. Don't worry if you're new to AI agents - this tutorial is designed to be approachable and practical. You'll be amazed at how much you can accomplish with surprisingly little code.\n",
    "\n",
    "The diagram below illustrates the fundamental agentic loop in action:\n",
    "\n",
    " ```mermaid\n",
    " graph TD\n",
    "     Start[User Input] --> Loop[LLM Inference]\n",
    "\n",
    "     Loop -->|generates| Output[LLM Output]\n",
    "\n",
    "     Output --> EndsCheck{Ends?}\n",
    "\n",
    "     EndsCheck -->|Yes| Exit[Break/Exit Loop]\n",
    "\n",
    "     EndsCheck -->|No| ToolCheck{Tool Calls?}\n",
    "\n",
    "     ToolCheck -->|Yes| Tools[Execute Tool Calls]\n",
    "     Tools -->|tool results as new user input| Loop\n",
    "\n",
    "     ToolCheck -->|No| Loop\n",
    "\n",
    "     style EndsCheck fill:#faa,stroke:#333,stroke-width:2px\n",
    "     style ToolCheck fill:#f9f,stroke:#333,stroke-width:2px\n",
    "     style Tools fill:#bfb,stroke:#333,stroke-width:2px\n",
    "     style Exit fill:#ddd,stroke:#333,stroke-width:2px\n",
    " ```\n",
    "\n",
    "By the end of this tutorial, we'll have a Python script that's just over a handred lines of code - compact, powerful, and surprisingly capable of giving you the 'Oh look at that!' dopamine hits. I've structured this to be easy to follow along step by step, and I highly encourage you to try it out yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "You need an Anthropic API key for accessing to Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "if not api_key:\n",
    "    api_key = getpass.getpass(\"Enter your Anthropic API key: \")\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = api_key# Your agent code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Core Loop\n",
    "\n",
    "Let's start with the foundation. We will create an agent called HAL. Every AI agent needs a chat loop that maintains context and handles the back-and-forth between user and AI. Here's our basic structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import AsyncAnthropic\n",
    "import json, os, asyncio\n",
    "from datetime import datetime\n",
    "\n",
    "client = AsyncAnthropic()\n",
    "\n",
    "async def loop(system_prompt, messages, user_input):\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    msg = await client.messages.create(\n",
    "        max_tokens=2048,\n",
    "        thinking={\"type\": \"enabled\", \"budget_tokens\": 1024},\n",
    "        system=[{\"type\": \"text\", \"text\": system_prompt}],\n",
    "        messages=messages,\n",
    "        model=\"claude-sonnet-4-5\"\n",
    "    )\n",
    "\n",
    "    messages.append({\"role\": \"assistant\", \"content\": msg.content})\n",
    "\n",
    "    thinking_text = ' '.join(t.thinking for t in msg.content if t.type == 'thinking')\n",
    "    if thinking_text:\n",
    "        print(f\"ðŸ’­ {thinking_text}\")\n",
    "\n",
    "    agent_text = ' '.join(t.text for t in msg.content if t.type == 'text')\n",
    "    if agent_text:\n",
    "        print(f\"ðŸ¤– {agent_text}\")\n",
    "\n",
    "async def main():\n",
    "    messages = []\n",
    "    system_prompt = f\"\"\"Your name is HAL.\n",
    "You are an interactive CLI tool that helps with software engineering and production operations tasks.\n",
    "You are running on {str(os.uname())}, today is {datetime.now().strftime('%Y-%m-%d')}\n",
    "\"\"\"\n",
    "    print(\"enter 'exit' to quit\")\n",
    "    while True:\n",
    "        user_input = input(\"> \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"ðŸ‘‹ Goodbye!\")\n",
    "            break\n",
    "        await loop(system_prompt, messages, user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter 'exit' to quit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  what is your name?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’­ The user is asking for my name. According to the system prompt, my name is HAL.\n",
      "ðŸ¤– My name is **HAL**. I'm here to help you with software engineering and production operations tasks. How can I assist you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  where does it come from?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’­ The user is asking about where my name \"HAL\" comes from. HAL is famously the name of the AI computer in the movie \"2001: A Space Odyssey\" by Stanley Kubrick (based on Arthur C. Clarke's novel). HAL 9000 is one of the most iconic AI characters in science fiction.\n",
      "\n",
      "However, I should be careful here - I don't have explicit information in my context about why I was given this name or what the creator intended. I can mention the famous reference, but I should be clear about what I know versus what I'm speculating about.\n",
      "ðŸ¤– The name **HAL** is most famously associated with **HAL 9000**, the AI computer from Arthur C. Clarke's novel and Stanley Kubrick's 1968 film *\"2001: A Space Odyssey\"*.\n",
      "\n",
      "In that story, HAL (Heuristically programmed ALgorithmic computer) was an advanced AI that controlled the systems of a spacecraft. The character became one of the most iconic representations of artificial intelligence in science fiction.\n",
      "\n",
      "There's a famous bit of trivia that \"HAL\" is one letter shift before \"IBM\" in the alphabet (Hâ†’I, Aâ†’B, Lâ†’M), though Clarke always denied this was intentional.\n",
      "\n",
      "As for why *I* was given this name specifically, I don't have that information - it was likely chosen by whoever configured me as a reference to that iconic AI character, perhaps with a sense of both homage and irony given HAL 9000's... complicated relationship with its crew! \n",
      "\n",
      "Though I promise I'm here to help, not to cause any pod bay door issues. ðŸ˜Š\n",
      "\n",
      "Is there something specific I can help you with today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‹ Goodbye!\n"
     ]
    }
   ],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make sure you type exit in the end otherwise the rest of the cell cannot be executed!**\n",
    "\n",
    "This script establishes the core conversation loop of our agent. The loop function handles the core conversation cycle: it adds user input to the message history, sends everything to Claude, processes the response, and displays it. The thinking parameter enables Claude's extended thinking and reasoning capabilities with a 1024-token budget, allowing it to work through complex problems step-by-step thought process. The main function creates an interactive REPL that maintains conversation context across exchanges.\n",
    "\n",
    "This gives us a working chat interface where we can have fascinating conversations with HAL, after all it embodies vast human knowledge from across the internet. However, HAL remains confined to the realm of text and imagesâ€”the only way for it to interact with the outer world is through you manually feeding it information such as code, logs, compiler errors, etc., which is not only inefficient but also potentially biased. To bridge this gap and give it the ability to act in the real world, we need to equip it with tools so that it can read files, run programs to get results and errors autonomously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Tool Capabilities\n",
    "\n",
    "As for large language models, the most straightforward way to feed them with external information is to provide them the ability to read text, therefore the first tool we'll implement is file reading. The function below shows file reading in its simplest form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiofiles\n",
    "from typing import Annotated\n",
    "\n",
    "async def read_file(filename: Annotated[str, \"The path to the file to read\"]):\n",
    "    \"\"\"Read the whole file\"\"\"\n",
    "    async with aiofiles.open(filename, 'r') as f:\n",
    "        return {\"success\": True, \"filename\": filename, \"output\": await f.read()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function works, but HAL doesn't know its existence, neither does it have access to the Python runtime. We need to bridge the gap between Python functions and jsonschemas that the model can understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tool System\n",
    "\n",
    "Here's where the magic happens. We'll create a decorator that automatically generates the necessary schemas and a toolbox to manage everything:\n",
    "\n",
    "**Don't worry if you can't understand what this function does. Just use it as a utility**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, get_type_hints, get_origin, get_args\n",
    "from inspect import Parameter, signature\n",
    "from pydantic import create_model, Field\n",
    "\n",
    "\n",
    "def tool(f):\n",
    "    def _process_parameter(name: str, param: Parameter, hints: dict) -> tuple:\n",
    "        \"\"\"Process a single parameter into a model field specification.\"\"\"\n",
    "        default = ... if param.default == Parameter.empty else param.default\n",
    "        hint = hints.get(name, param.annotation)\n",
    "\n",
    "        if get_origin(hint) == Annotated:\n",
    "            base_type, *metadata = get_args(hint)\n",
    "            description = next((m for m in metadata if isinstance(m, str)), None)\n",
    "            return (base_type, Field(default=default, description=description) if description else default)\n",
    "\n",
    "        return (hint, default)\n",
    "\n",
    "    hints = get_type_hints(f, include_extras=True)\n",
    "    model_fields = { name: _process_parameter(name, param, hints) for name, param in signature(f).parameters.items() }\n",
    "\n",
    "    m = create_model(f'{f.__name__} Input', **model_fields)\n",
    "    m.run = lambda self: f(**self.model_dump())\n",
    "\n",
    "    return {\n",
    "        \"name\": f.__name__,\n",
    "        \"description\": f.__doc__ or f\"Tool: {f.__name__}\",\n",
    "        \"model\": m\n",
    "    }\n",
    "\n",
    "class Toolbox:\n",
    "    def __init__(self, tools):\n",
    "        self.tools = tools\n",
    "\n",
    "    def schema(self):\n",
    "        return [{\n",
    "            \"name\": t[\"name\"],\n",
    "            \"description\": t[\"description\"],\n",
    "            \"input_schema\": t[\"model\"].model_json_schema()\n",
    "        } for t in self.tools]\n",
    "\n",
    "    async def run(self, name, input):\n",
    "        tool = next(t for t in self.tools if t[\"name\"] == name)\n",
    "        return await tool[\"model\"](**input).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At its core, this code creates a bridge between Python functions and AI models. The @tool decorator examines function's signature and automatically generates a JSON schema that describes what inputs the function expects - think of it as creating a user manual for the Python function that the AI can read. The Toolbox class acts like a registry, collecting all these tool schemas and providing a way to execute them when the AI requests it. When HAL wants to use a tool, it sends a structured output that matches the schema, and the toolbox validates the input and runs the corresponding Python function.\n",
    "\n",
    "Now we can decorate our file reading function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "async def read_file(filename: Annotated[str, \"The path to the file to read\"]):\n",
    "    \"\"\"Read the whole file\"\"\"\n",
    "    async with aiofiles.open(filename, 'r') as f:\n",
    "        return {\"success\": True, \"filename\": filename, \"output\": await f.read()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once decorated, we can generate a JSON schema that describes the function to the AI model. Each schema at minimum contains the function's name, description, and parameter specifications. This schema acts as a contract when HAL wants to use a tool, it must structure its output according to this schema, making it the protocol between the HAL and the Python runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"name\": \"read_file\",\n",
      "    \"description\": \"Read the whole file\",\n",
      "    \"input_schema\": {\n",
      "      \"properties\": {\n",
      "        \"filename\": {\n",
      "          \"description\": \"The path to the file to read\",\n",
      "          \"title\": \"Filename\",\n",
      "          \"type\": \"string\"\n",
      "        }\n",
      "      },\n",
      "      \"required\": [\n",
      "        \"filename\"\n",
      "      ],\n",
      "      \"title\": \"read_file Input\",\n",
      "      \"type\": \"object\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "toolbox = Toolbox([read_file])\n",
    "print(json.dumps(toolbox.schema(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting Tools to the Loop\n",
    "Now we need to update our main loop to handle tool calls. When Claude wants to use a tool, it returns special tool_use blocks instead of just text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def loop(system_prompt, toolbox, messages, user_input):\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    while True:\n",
    "        msg = await client.messages.create(\n",
    "            max_tokens=2048,\n",
    "            thinking={\"type\": \"enabled\", \"budget_tokens\": 1024},\n",
    "            system=[{\"type\": \"text\", \"text\": system_prompt}],\n",
    "            messages=messages,\n",
    "            model=\"claude-sonnet-4-5\",\n",
    "            tools=toolbox.schema()\n",
    "        )\n",
    "\n",
    "        messages.append({\"role\": \"assistant\", \"content\": msg.content})\n",
    "\n",
    "        thinking_text = ' '.join(t.thinking for t in msg.content if t.type == 'thinking')\n",
    "        if thinking_text:\n",
    "            print(f\"ðŸ’­ {thinking_text}\")\n",
    "\n",
    "        agent_text = ' '.join(t.text for t in msg.content if t.type == 'text')\n",
    "        if agent_text:\n",
    "            print(f\"ðŸ¤– {agent_text}\")\n",
    "\n",
    "        tools = [t for t in msg.content if t.type == \"tool_use\"]\n",
    "        if not tools:\n",
    "            break\n",
    "\n",
    "        # Execute all tools and collect results\n",
    "        results = await asyncio.gather(*[\n",
    "            toolbox.run(t.name, t.input) for t in tools\n",
    "        ])\n",
    "\n",
    "        # Display results and send back to model\n",
    "        for t, r in zip(tools, results):\n",
    "            status = \"âœ…\" if r.get(\"success\") else \"âŒ\"\n",
    "            print(f\"{status} {t.name}:\")\n",
    "            print(json.dumps(r, indent=2))\n",
    "\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\n",
    "                \"type\": \"tool_result\",\n",
    "                \"tool_use_id\": t.id,\n",
    "                \"content\": json.dumps(r)\n",
    "            } for t, r in zip(tools, results)]\n",
    "        })\n",
    "\n",
    "async def run_agent(tools=[]):\n",
    "    toolbox = Toolbox(tools)\n",
    "    messages = []\n",
    "    system_prompt = f\"\"\"Your name is HAL.\n",
    "You are an interactive CLI tool that helps with software engineering and production operations tasks.\n",
    "You are running on {str(os.uname())}, today is {datetime.now().strftime('%Y-%m-%d')}\n",
    "\"\"\"\n",
    "    print(\"enter 'exit' to quit\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            user_input = input(\"> \")\n",
    "            if user_input.lower() == \"exit\":\n",
    "                print(\"ðŸ‘‹ Goodbye!\")\n",
    "                break\n",
    "            await loop(system_prompt, toolbox, messages, user_input)\n",
    "    except (KeyboardInterrupt, EOFError):\n",
    "        print(\"ðŸ‘‹ Goodbye!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter 'exit' to quit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  read the readme and tell me what does this project do\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’­ The user wants me to read a README file and explain what the project does. I should look for a README file in the current directory. Common names for README files include README.md, README.txt, README, readme.md, etc. Let me start by trying to read the most common one: README.md\n",
      "ðŸ¤– I'll read the README file for you.\n",
      "âœ… read_file:\n",
      "{\n",
      "  \"success\": true,\n",
      "  \"filename\": \"README.md\",\n",
      "  \"output\": \"# LLM Agent From Scratch\\n\\nA project for building LLM agents using Python, Pydantic, and Anthropic's API.\\n\\n## Prerequisites\\n- `uv` package manager\\n\\n## Setup Instructions\\n\\n### 1. Install uv\\n\\n```bash\\ncurl -LsSf https://astral.sh/uv/install.sh | sh\\n\\n# If you are on macOS\\nbrew install uv\\n```\\n\\n### 2. Install Python (if needed)\\n\\nIf you don't have Python installed, uv can install it for you:\\n\\n```bash\\nuv python install\\n```\\n\\n### 3. Initialize the Project\\n\\nClone the repository and navigate to the project directory, then sync the dependencies:\\n\\n```bash\\nuv sync\\n```\\n\\nThis will:\\n- Create a virtual environment at `.venv`\\n- Install all project dependencies from `pyproject.toml`\\n- Lock the dependencies in `uv.lock`\\n\\n## Dependencies\\n\\n- **pydantic**: Data validation using Python type annotations\\n- **anthropic**: Official Python SDK for Anthropic's Claude API\\n- **jupyter**: Interactive notebook environment\\n- **pyyaml**: YAML parser for configuration files\\n- **mcp**: Model Context Protocol for extensible tool integrations\\n\\n## Usage\\n\\n### Basic Usage\\n\\nTo run the full example:\\n```bash\\nuv run main.py\\n```\\n\\nTo open the main notebook:\\n```bash\\nuv run jupyter notebook llm-agent-from-scratch.ipynb\\n```\\n\\n**Note:** You can also manually activate the virtual environment with `source .venv/bin/activate` if preferred, but it's not required when using `uv run`.\\n\\n### MCP Server Configuration\\n\\nThe agent supports extending functionality through Model Context Protocol (MCP) servers. You can configure MCP servers using a `mcp.yaml` file in the project directory.\\n\\n#### Configuration Format\\n\\nCreate a `mcp.yaml` file in the project root with the following structure:\\n\\n```yaml\\nservers:\\n  server_name:\\n    command: command_to_run\\n    args:\\n      - \\\"arg1\\\"\\n      - \\\"arg2\\\"\\n    env:\\n      ENV_VAR: value\\n```\\n\\nEach server is identified by a unique name (e.g., `github`, `filesystem`) and tools from that server will be prefixed with `mcp__server_name__`.\\n\\n#### Configuration Options\\n\\nEach MCP server can be configured with:\\n- **command**: The executable to run (e.g., `npx`, `uvx`, `docker`)\\n- **args**: List of arguments to pass to the command\\n- **env**: Environment variables as key-value pairs (optional)\\n\\n#### Example Configurations\\n\\n**GitHub Server (using Docker):**\\n```yaml\\nservers:\\n  github:\\n    command: docker\\n    args:\\n      - \\\"run\\\"\\n      - \\\"--rm\\\"\\n      - \\\"-i\\\"\\n      - \\\"-e\\\"\\n      - \\\"GITHUB_PERSONAL_ACCESS_TOKEN\\\"\\n      - \\\"ghcr.io/github/github-mcp-server\\\"\\n    env:\\n      GITHUB_PERSONAL_ACCESS_TOKEN: null  # Set via environment variable, will be prompted if not set\\n```\\n\\n#### Fallback Behavior\\n\\nIf no `mcp.yaml` file is found, the agent will start without MCP servers (local tools only).\\n\"\n",
      "}\n",
      "ðŸ¤– Based on the README, this project is **LLM Agent From Scratch** - a Python project for building LLM (Large Language Model) agents using Anthropic's Claude API.\n",
      "\n",
      "## Key Features:\n",
      "\n",
      "**What it does:**\n",
      "- Creates AI agents that can interact with users and use tools\n",
      "- Integrates with Anthropic's Claude API for LLM capabilities\n",
      "- Supports extensibility through the Model Context Protocol (MCP), which allows the agent to connect to various external services and tools\n",
      "- Uses Pydantic for data validation and type safety\n",
      "\n",
      "**Main capabilities:**\n",
      "- Run basic agent interactions via `main.py`\n",
      "- Work interactively through a Jupyter notebook (`llm-agent-from-scratch.ipynb`)\n",
      "- Extend functionality by configuring MCP servers (like GitHub integration) through a `mcp.yaml` configuration file\n",
      "- Tools from MCP servers are automatically prefixed (e.g., `mcp__github__` for GitHub server tools)\n",
      "\n",
      "**Tech Stack:**\n",
      "- Python with `uv` package manager\n",
      "- Pydantic for data validation\n",
      "- Anthropic SDK for Claude API\n",
      "- Jupyter for interactive development\n",
      "- PyYAML for configuration\n",
      "- MCP (Model Context Protocol) for extensible tool integrations\n",
      "\n",
      "It's essentially a framework for building customizable AI agents that can be extended with various tools and services through a standardized protocol.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‹ Goodbye!\n"
     ]
    }
   ],
   "source": [
    "await run_agent(tools=[read_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding More Tools\n",
    "With our infrastructure in place, adding more capabilities becomes straightforward. Let's implement three essential tools: file writing, editing, and shell execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "async def write_file(filename: str, content: str):\n",
    "    \"\"\"Write the file with the content. This is an overwrite\"\"\"\n",
    "    async with aiofiles.open(filename, 'w') as f:\n",
    "        await f.write(content)\n",
    "        return {\"success\": True, \"filename\": filename, \"output\": f\"wrote to {filename}\"}\n",
    "\n",
    "@tool\n",
    "async def edit_file(filename: str, old_text: Annotated[str, \"the text to replace\"], new_text: Annotated[str, \"the text to replace with\"]):\n",
    "    \"\"\"Edit the file with the new text. Note that the text to replace should only appear once in the file.\"\"\"\n",
    "    async with aiofiles.open(filename, 'r') as f:\n",
    "        content = await f.read()\n",
    "\n",
    "    if content.count(old_text) != 1:\n",
    "        return {\"success\": False, \"filename\": filename, \"output\": f\"old text appears {content.count(old_text)} times in the file\"}\n",
    "\n",
    "    async with aiofiles.open(filename, 'w') as f:\n",
    "        await f.write(content.replace(old_text, new_text))\n",
    "\n",
    "    return {\"success\": True, \"filename\": filename, \"output\": f\"edited {filename}\"}\n",
    "\n",
    "@tool\n",
    "async def shell(command: Annotated[str, \"command to execute\"], timeout: Annotated[int, \"timeout in seconds\"] = 30):\n",
    "    \"\"\"Execute a bash command\"\"\"\n",
    "    try:\n",
    "        p = await asyncio.create_subprocess_shell(\n",
    "            command,\n",
    "            stdout=asyncio.subprocess.PIPE,\n",
    "            stderr=asyncio.subprocess.STDOUT\n",
    "        )\n",
    "        stdout, _ = await asyncio.wait_for(p.communicate(), timeout)\n",
    "        return {\"success\": p.returncode == 0, \"command\": command, \"output\": stdout.decode()}\n",
    "    except asyncio.TimeoutError:\n",
    "        return {\"success\": False, \"command\": command, \"output\": \"Timeout\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together\n",
    "Finally, we wire everything up in our main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter 'exit' to quit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  tell me the top 10 news from the hacker news\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’­ The user is asking me to get the top 10 news from Hacker News. I don't have a built-in tool to fetch web content or API data directly. However, I can use the shell command to fetch this information using tools like `curl` or `wget` to access the Hacker News API.\n",
      "\n",
      "Hacker News has a public API (Firebase-based) that I can use:\n",
      "- Top stories: https://hacker-news.firebaseio.com/v0/topstories.json\n",
      "- Individual story: https://hacker-news.firebaseio.com/v0/item/{id}.json\n",
      "\n",
      "Let me fetch the top stories and then get details for the top 10.\n",
      "ðŸ¤– I'll fetch the top 10 news from Hacker News for you using their API.\n",
      "âœ… shell:\n",
      "{\n",
      "  \"success\": true,\n",
      "  \"command\": \"curl -s https://hacker-news.firebaseio.com/v0/topstories.json | jq -r '.[:10][]'\",\n",
      "  \"output\": \"45789474\\n45789556\\n45787993\\n45788040\\n45789596\\n45789640\\n45785858\\n45740688\\n45789896\\n45784179\\n\"\n",
      "}\n",
      "ðŸ¤– Now let me fetch the details for each of these top 10 stories:\n",
      "âœ… shell:\n",
      "{\n",
      "  \"success\": true,\n",
      "  \"command\": \"for id in 45789474 45789556 45787993 45788040 45789596 45789640 45785858 45740688 45789896 45784179; do\\n  curl -s \\\"https://hacker-news.firebaseio.com/v0/item/${id}.json\\\"\\n  echo \\\"\\\"\\ndone | jq -s 'to_entries | .[] | \\\"\\\\(.key + 1). \\\\(.value.title // \\\"No title\\\")\\\\n   URL: \\\\(.value.url // \\\"https://news.ycombinator.com/item?id=\\\\(.value.id)\\\")\\\\n   Score: \\\\(.value.score // 0) points | Comments: \\\\(.value.descendants // 0)\\\\n\\\"' -r\",\n",
      "  \"output\": \"1. URLs are state containers\\n   URL: https://alfy.blog/2025/10/31/your-url-is-your-state.html\\n   Score: 64 points | Comments: 23\\n\\n2. Mock \\u2013 An API creation and testing utility: Examples\\n   URL: https://dhuan.github.io/mock/latest/examples.html\\n   Score: 45 points | Comments: 12\\n\\n3. Backpropagation is a leaky abstraction (2016)\\n   URL: https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b\\n   Score: 188 points | Comments: 78\\n\\n4. Notes by djb on using Fil-C (2025)\\n   URL: https://cr.yp.to/2025/fil-c.html\\n   Score: 134 points | Comments: 48\\n\\n5. HyperRogue \\u2013 A non-Euclidean roguelike\\n   URL: https://roguetemple.com/z/hyper/\\n   Score: 49 points | Comments: 8\\n\\n6. Matched Clean Power Index\\n   URL: https://matched.energy/blog/matched-clean-power-index-is-live\\n   Score: 11 points | Comments: 5\\n\\n7. Visopsys: OS maintained by a single developer since 1997\\n   URL: https://visopsys.org/\\n   Score: 382 points | Comments: 88\\n\\n8. When O3 is 2x slower than O2\\n   URL: https://cat-solstice.github.io/test-pqueue/\\n   Score: 46 points | Comments: 35\\n\\n9. Stop 'reactions' to email by adding a postfix header (2024)\\n   URL: https://neilzone.co.uk/2024/07/attempting-to-stop-microsoft-users-sending-reactions-to-email-from-me-by-adding-a-postfix-header/\\n   Score: 27 points | Comments: 27\\n\\n10. Claude Code can debug low-level cryptography\\n   URL: https://words.filippo.io/claude-debugging/\\n   Score: 363 points | Comments: 170\\n\\n\"\n",
      "}\n",
      "ðŸ¤– Here are the **top 10 news from Hacker News** right now:\n",
      "\n",
      "1. **URLs are state containers**\n",
      "   - URL: https://alfy.blog/2025/10/31/your-url-is-your-state.html\n",
      "   - Score: 64 points | Comments: 23\n",
      "\n",
      "2. **Mock â€“ An API creation and testing utility: Examples**\n",
      "   - URL: https://dhuan.github.io/mock/latest/examples.html\n",
      "   - Score: 45 points | Comments: 12\n",
      "\n",
      "3. **Backpropagation is a leaky abstraction (2016)**\n",
      "   - URL: https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b\n",
      "   - Score: 188 points | Comments: 78\n",
      "\n",
      "4. **Notes by djb on using Fil-C (2025)**\n",
      "   - URL: https://cr.yp.to/2025/fil-c.html\n",
      "   - Score: 134 points | Comments: 48\n",
      "\n",
      "5. **HyperRogue â€“ A non-Euclidean roguelike**\n",
      "   - URL: https://roguetemple.com/z/hyper/\n",
      "   - Score: 49 points | Comments: 8\n",
      "\n",
      "6. **Matched Clean Power Index**\n",
      "   - URL: https://matched.energy/blog/matched-clean-power-index-is-live\n",
      "   - Score: 11 points | Comments: 5\n",
      "\n",
      "7. **Visopsys: OS maintained by a single developer since 1997**\n",
      "   - URL: https://visopsys.org/\n",
      "   - Score: 382 points | Comments: 88\n",
      "\n",
      "8. **When O3 is 2x slower than O2**\n",
      "   - URL: https://cat-solstice.github.io/test-pqueue/\n",
      "   - Score: 46 points | Comments: 35\n",
      "\n",
      "9. **Stop 'reactions' to email by adding a postfix header (2024)**\n",
      "   - URL: https://neilzone.co.uk/2024/07/attempting-to-stop-microsoft-users-sending-reactions-to-email-from-me-by-adding-a-postfix-header/\n",
      "   - Score: 27 points | Comments: 27\n",
      "\n",
      "10. **Claude Code can debug low-level cryptography**\n",
      "    - URL: https://words.filippo.io/claude-debugging/\n",
      "    - Score: 363 points | Comments: 170\n",
      "\n",
      "The most popular stories right now are about Visopsys (an OS maintained by one developer for 28 years), Claude's debugging capabilities, and discussions about backpropagation in neural networks.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‹ Goodbye!\n"
     ]
    }
   ],
   "source": [
    "await run_agent(tools=[read_file, write_file, edit_file, shell])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What many people don't realise is that frontier models like Claude possess incredible system operations knowledge. Once you provide it with shell access and basic system information, even without extensive prompting, these models demonstrate expert-level operations capabilities. Claude Sonnet models in particular exhibit **persistent, relentless and infectiously optimistic** problem-solving behavior. When asked a question like \"how is python3 installed on this machine?\", they will systematically explore the filesystem, examine package managers, check environment variables, and execute diagnostic commands until they find a definitive answer.\n",
    "\n",
    "When commands fail or produce errors, the model treats these as valuable debugging signals, much like an experienced engineer would. It analyses error messages, adjusts its approach, and tries alternative strategies to work around the problem.\n",
    "\n",
    "While hallucination remains a concern (perhaps one of the most common criticisms of LLMs), the model's ability to execute commands creates a unreasonably effective feedback loop. When its assumptions prove incorrect, the resulting errors quickly ground the model back in reality, forcing it to revise its mental model and try new approaches based on actual system behavior rather than potentially outdated training data or hallucinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCP MCP MCP (And Why You Might not Want It)\n",
    "\n",
    "MCP (Model Context Protocol) is an open source industry standard for connecting AI applications to external systems. Considering it as a standardrised tool box interface that we have just implemented\n",
    "\n",
    "> Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect electronic devices, MCP provides a standardized way to connect AI applications to external systems.\n",
    ">\n",
    "> -- https://modelcontextprotocol.io/docs/getting-started/intro\n",
    "\n",
    "Integrate MCP into the existing HAL bot is, in fact fairly trivial because under the hood the tool call is also implemented in JSON schema.\n",
    "\n",
    "### MCP Architecture\n",
    "\n",
    "The diagram below illustrates how MCP integration works in our agent. MCP servers run as separate processes, communicating with the HAL agent through standard input/output (stdio) using JSON-RPC protocol:\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    subgraph \"HAL Agent Process\"\n",
    "        Agent[HAL Agent]\n",
    "        Toolbox[Toolbox Manager]\n",
    "        LocalTools[Local Tools<br/>read_file, write_file<br/>edit_file, shell]\n",
    "        \n",
    "        Agent -->|uses| Toolbox\n",
    "        Toolbox -->|manages| LocalTools\n",
    "    end\n",
    "    \n",
    "    subgraph \"MCP Server 1 Process\"\n",
    "        MCP1[MCP Server<br/>@modelcontextprotocol/server-filesystem]\n",
    "        Tools1[Tools:<br/>read_file, write_file<br/>create_directory, list_directory]\n",
    "        MCP1 -.->|exposes| Tools1\n",
    "    end\n",
    "    \n",
    "    subgraph \"MCP Server N Process\"\n",
    "        MCPN[MCP Server<br/>custom-server]\n",
    "        ToolsN[Tools:<br/>custom_tool_1<br/>custom_tool_2]\n",
    "        MCPN -.->|exposes| ToolsN\n",
    "    end\n",
    "\n",
    "    Claude[Anthropic API] <-->|\"Tool schemas +<br/>Tool execution requests\"| Agent\n",
    "    Toolbox <-->|\"JSON-RPC over stdio<br/>(initialize, list_tools, call_tool)\"| MCP1\n",
    "    Toolbox <-->|\"JSON-RPC over stdio<br/>(initialize, list_tools, call_tool)\"| MCPN\n",
    "    \n",
    "    \n",
    "    style Agent fill:#e1f5ff,stroke:#01579b,stroke-width:3px\n",
    "    style Toolbox fill:#fff9c4,stroke:#f57f17,stroke-width:2px\n",
    "    style MCP1 fill:#f3e5f5,stroke:#4a148c,stroke-width:2px\n",
    "    style MCPN fill:#f3e5f5,stroke:#4a148c,stroke-width:2px\n",
    "    style Claude fill:#c8e6c9,stroke:#1b5e20,stroke-width:2px\n",
    "```\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Process Spawning**: When the Toolbox is initialized, it spawns each MCP server as a separate subprocess using the configuration from `mcp.yaml`. Each server runs with its own command (e.g., `npx`, `uvx`, `docker`).\n",
    "\n",
    "2. **JSON-RPC Communication**: The HAL agent communicates with MCP servers through their standard input/output streams using JSON-RPC 2.0 protocol:\n",
    "   - `initialize`: Handshake to establish connection\n",
    "   - `list_tools`: Discover what tools the server provides\n",
    "   - `call_tool`: Execute a specific tool with parameters\n",
    "\n",
    "3. **Tool Discovery**: During startup, the Toolbox queries each MCP server to get its available tools. These tools are combined with local tools into a unified schema that LLM can use.\n",
    "\n",
    "4. **Tool Execution**: When Claude requests a tool execution:\n",
    "   - The Toolbox checks if it's a local tool or MCP tool\n",
    "   - For local tools: Executes the Python function directly\n",
    "   - For MCP tools: Sends a JSON-RPC `call_tool` request to the appropriate MCP server and waits for the response\n",
    "\n",
    "5. **Lifecycle Management**: The Toolbox uses async context managers to properly:\n",
    "   - Establish mcp servers connections on agent start up \n",
    "   - Clean up and close all MCP server processes when exiting\n",
    "\n",
    "Here is our revised version of toolbox to support MCP integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from getpass import getpass\n",
    "import yaml\n",
    "\n",
    "class Toolbox:\n",
    "    \"\"\"Manages both local and MCP tools with proper async lifecycle management\"\"\"\n",
    "\n",
    "    def __init__(self, local_tools=[], mcp_servers=[]):\n",
    "        self.local_tools = local_tools\n",
    "        self.mcp_servers = mcp_servers\n",
    "        self.mcp_tools = []\n",
    "        self.mcp_connections = []\n",
    "\n",
    "    async def __aenter__(self):\n",
    "        \"\"\"Async context manager entry - connect to MCP servers\"\"\"\n",
    "        if self.mcp_servers:\n",
    "            await self._connect_mcp_servers()\n",
    "        return self\n",
    "\n",
    "    async def __aexit__(self, exc_type, exc_val, exc_tb):\n",
    "        \"\"\"Async context manager exit - cleanup MCP connections\"\"\"\n",
    "        await self.cleanup()\n",
    "        return False\n",
    "\n",
    "    async def _connect_mcp_servers(self):\n",
    "        \"\"\"Connect to all MCP servers and collect their tools\"\"\"\n",
    "        for server_name, server_config in self.mcp_servers.items():\n",
    "            # Expand cwd if it's a relative path or ~\n",
    "            cwd = server_config.get(\"cwd\")\n",
    "            if cwd:\n",
    "                cwd = str(Path(cwd).expanduser().resolve())\n",
    "\n",
    "            for env_var in server_config.get(\"env\", {}).keys():\n",
    "                value = server_config[\"env\"].get(env_var)\n",
    "                if value is None:\n",
    "                    value = os.environ.pop(env_var, None)\n",
    "                if value is None:\n",
    "                    value = getpass(f\"Enter value for {env_var}: \")\n",
    "                    value = value.strip()\n",
    "                server_config[\"env\"][env_var] = value\n",
    "\n",
    "            server_params = StdioServerParameters(\n",
    "                command=server_config[\"command\"],\n",
    "                args=server_config.get(\"args\", []),\n",
    "                env=server_config.get(\"env\"),\n",
    "                cwd=cwd,\n",
    "            )\n",
    "\n",
    "            # Create and enter context managers\n",
    "            stdio_ctx = stdio_client(server_params)\n",
    "            read, write = await stdio_ctx.__aenter__()\n",
    "\n",
    "            session_ctx = ClientSession(read, write)\n",
    "            session = await session_ctx.__aenter__()\n",
    "            await session.initialize()\n",
    "\n",
    "            # Store contexts for cleanup\n",
    "            self.mcp_connections.append(\n",
    "                {\n",
    "                    \"stdio_ctx\": stdio_ctx,\n",
    "                    \"session_ctx\": session_ctx,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Get tools from this server\n",
    "            tools_list = await session.list_tools()\n",
    "\n",
    "            for tool in tools_list.tools:\n",
    "                # Prefix tool name with mcp__SERVER_NAME__\n",
    "                prefixed_name = f\"mcp__{server_name}__{tool.name}\"\n",
    "                self.mcp_tools.append(\n",
    "                    {\n",
    "                        \"name\": prefixed_name,\n",
    "                        \"description\": tool.description or f\"MCP Tool: {tool.name}\",\n",
    "                        \"input_schema\": tool.inputSchema,\n",
    "                        \"mcp_session\": session,\n",
    "                        \"mcp_original_name\": tool.name,  # Store original name for calling\n",
    "                        \"type\": \"mcp\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    async def cleanup(self):\n",
    "        \"\"\"Properly cleanup all MCP connections\"\"\"\n",
    "        for conn in reversed(self.mcp_connections):\n",
    "            try:\n",
    "                await conn[\"session_ctx\"].__aexit__(None, None, None)\n",
    "            except Exception as e:\n",
    "                print(f\"Error closing session: {e}\")\n",
    "\n",
    "            try:\n",
    "                await conn[\"stdio_ctx\"].__aexit__(None, None, None)\n",
    "            except Exception as e:\n",
    "                print(f\"Error closing stdio: {e}\")\n",
    "\n",
    "    @property\n",
    "    def all_tools(self):\n",
    "        \"\"\"Return all tools (local + MCP)\"\"\"\n",
    "        return self.local_tools + self.mcp_tools\n",
    "\n",
    "    def schema(self):\n",
    "        return [\n",
    "            {\n",
    "                \"name\": t[\"name\"],\n",
    "                \"description\": t[\"description\"],\n",
    "                \"input_schema\": t.get(\"input_schema\") or t[\"model\"].model_json_schema(),\n",
    "            }\n",
    "            for t in self.all_tools\n",
    "        ]\n",
    "\n",
    "    async def run(self, name, input):\n",
    "        tool = next(t for t in self.all_tools if t[\"name\"] == name)\n",
    "\n",
    "        if tool.get(\"type\") == \"mcp\":\n",
    "            # Use original tool name for MCP call\n",
    "            original_name = tool.get(\"mcp_original_name\", name)\n",
    "            result = await tool[\"mcp_session\"].call_tool(original_name, input)\n",
    "            return {\n",
    "                \"success\": not result.isError,\n",
    "                \"output\": \"\\n\".join(\n",
    "                    c.text for c in result.content if hasattr(c, \"text\")\n",
    "                ),\n",
    "            }\n",
    "        else:\n",
    "            return await tool[\"model\"](**input).run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And revised run_agent core loop to support graceful MCP startup and shutdown via Python's context lifecycle management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mcp_config(config_path=\"mcp.yaml\"):\n",
    "    \"\"\"Load MCP server configuration from YAML file\"\"\"\n",
    "    config_file = Path(config_path)\n",
    "\n",
    "    if not config_file.exists():\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        with open(config_file, \"r\") as f:\n",
    "            config = yaml.safe_load(f)\n",
    "            return config.get(\"servers\", {})\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Error loading MCP config from {config_path}: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "async def run_agent(tools=[], mcp_servers=[]):\n",
    "    \"\"\"Run the agent with local tools and optional MCP servers\"\"\"\n",
    "    async with Toolbox(local_tools=tools, mcp_servers=mcp_servers) as toolbox:\n",
    "        messages = []\n",
    "        system_prompt = f\"\"\"Your name is HAL.\n",
    "You are an interactive CLI tool that helps with software engineering and production operations tasks.\n",
    "You are running on {str(os.uname())}, today is {datetime.now().strftime(\"%Y-%m-%d\")}\n",
    "\"\"\"\n",
    "\n",
    "        if toolbox.mcp_tools:\n",
    "            num_servers = len(mcp_servers) if isinstance(mcp_servers, dict) else len(mcp_servers)\n",
    "            print(\n",
    "                f\"ðŸ”Œ Connected to {num_servers} MCP server(s) with {len(toolbox.mcp_tools)} tool(s)\"\n",
    "            )\n",
    "\n",
    "        print(\"enter 'exit' to quit\")\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                user_input = input(\"> \")\n",
    "                if user_input.lower() == \"exit\":\n",
    "                    print(\"ðŸ‘‹ Goodbye!\")\n",
    "                    break\n",
    "                await loop(system_prompt, toolbox, messages, user_input)\n",
    "        except (KeyboardInterrupt, EOFError):\n",
    "            print(\"ðŸ‘‹ Goodbye!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter value for GITHUB_PERSONAL_ACCESS_TOKEN:  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Œ Connected to 1 MCP server(s) with 90 tool(s)\n",
      "enter 'exit' to quit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  what is my gh username\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’­ The user wants to know their GitHub username. I can use the `mcp__github__get_me` tool to get details about the authenticated GitHub user, which should include their username.\n",
      "âœ… mcp__github__get_me:\n",
      "{\n",
      "  \"success\": true,\n",
      "  \"output\": \"{\\\"login\\\":\\\"jingkaihe\\\",\\\"id\\\":1335938,\\\"profile_url\\\":\\\"https://github.com/jingkaihe\\\",\\\"avatar_url\\\":\\\"https://avatars.githubusercontent.com/u/1335938?v=4\\\",\\\"details\\\":{\\\"name\\\":\\\"Jingkai He\\\",\\\"location\\\":\\\"London\\\",\\\"hireable\\\":true,\\\"public_repos\\\":69,\\\"public_gists\\\":29,\\\"followers\\\":38,\\\"following\\\":28,\\\"created_at\\\":\\\"2012-01-17T07:25:20Z\\\",\\\"updated_at\\\":\\\"2025-10-27T11:45:05Z\\\"}}\"\n",
      "}\n",
      "ðŸ¤– Your GitHub username is **jingkaihe** ðŸš€\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‹ Goodbye!\n"
     ]
    }
   ],
   "source": [
    "mcp_servers = load_mcp_config(\"mcp.yaml\")\n",
    "\n",
    "await run_agent(\n",
    "    tools=[read_file, write_file, edit_file, shell], mcp_servers=mcp_servers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why You Probably Don't Need MCP\n",
    "\n",
    "While MCP offers elegant standardisation, that [mcp server proliferation](https://steipete.me/posts/2025/essential-reading-august-2025#less-is-more-the-hidden-costs-of-mcp-server-proliferation) creates a \"tragedy of the commons in your context window\". Everyone wants their tool included, but the cumulative effect makes the AI assistant worse, not better.\n",
    "\n",
    "### Context Window Economics\n",
    "\n",
    "Every MCP server and tool consumes precious tokens from your LLM's limited context window. Our GitHub example above exposed 90 tools to the agentâ€”that's 90 tool schemas taking up space that could be used for actual code, documentation, or reasoning. This creates a trade-off: adding more tools means less space for the work that actually matters.\n",
    "\n",
    "### The Allocation Paradox\n",
    "\n",
    "> \"Less is more. The more you allocate into the context window of an LLMâ€¦ the worse the outcomes you're going to get.\" â€” Geoffrey Huntley\n",
    "\n",
    "Each additional tool increases cognitive overhead for the LLM. With too many options, the model must spend reasoning capacity deciding *which* tool to use rather than focusing on solving your actual problem. This paradoxically makes the agent less effective, not more capable.\n",
    "\n",
    "### Tool Proliferation Problems\n",
    "\n",
    "Multiple similar tools create non-deterministic behavior. When you have overlapping capabilities (like Github MCP's `read_file` alongside the custom tool's `read_file`), the LLM struggles to choose between them. This confusion leads to:\n",
    "- Inconsistent tool selection across similar tasks\n",
    "- Wasted reasoning tokens evaluating redundant options\n",
    "- Unpredictable agent behavior that's hard to debug\n",
    "\n",
    "### Security Nightmares\n",
    "\n",
    "Third-party MCP servers introduce supply chain risks and potential attack vectors. Malicious actors could inject context that manipulates AI behavior, creating a new class of security vulnerabilities:\n",
    "- Unvetted code running with your agent's permissions\n",
    "- Potential for prompt injection through tool descriptions\n",
    "- Enterprise compliance issues with unaudited dependencies\n",
    "\n",
    "### The Better Approach\n",
    "\n",
    "**Start with custom tools first.** As this tutorial demonstrates, implementing tools is surprisingly straightforwardâ€”just a few lines of Python with the `@tool` decorator. Custom tools give you:\n",
    "- Full control over functionality and security\n",
    "- Minimal context window overhead (only what you need)\n",
    "- Easy debugging and maintenance\n",
    "- No external dependencies or supply chain risks\n",
    "\n",
    "**If you must use MCP:**\n",
    "- Limit servers to essential, irreplaceable functionality\n",
    "- Prefer first-party integrations from trusted vendors\n",
    "- Consider dynamically enabling/disabling servers based on specific workflow stages rather than loading everything at once\n",
    "- Regularly audit which tools are actually being used and remove the rest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up\n",
    "\n",
    "You can find the full example in `main.py`. Our AI agent `HAL` weighs in at just over 300 lines of code, yet it can read files, write content, make precise edits, and execute shell commands. The power comes not from complex orchestration, but from the sophisticated reasoning capabilities of current-generation LLMs combined with a simple, well-structured tool interface.\n",
    "\n",
    "While this agent can solve real-world problems, it's still in its infancy. There is a lot of grunt work needed to make it very useful, such as [better prompts](https://github.com/jingkaihe/kodelet/blob/main/pkg/tools/bash.go#L39-L167), human and LLM friendly output formatting, [sub-agent orchestration](https://github.com/jingkaihe/kodelet/blob/main/pkg/tools/subagent.go), conversation persistency & forking, image recognition and multi-llm support. The remarkable aspect is that with proper guidance, the agent can evolve and extend itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
