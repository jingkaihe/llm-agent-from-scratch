{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an LLM Agent from Scratch\n",
    "\n",
    "It looks like magic when you witness AI agents autonomously viewing files, making edits, executing commands, and working around errors. However, under the hood, the core loop is surprisingly simple: it runs a continuous cycle that takes user input, generates tool calls and text output, receives feedback from tool execution, feeds the result back to the LLM, and repeats. The complexity lies not in any orchestration, but in the sophisticated reasoning capabilities of the frontier labs' large language models.\n",
    "\n",
    "To demystify this magic, we'll implement a simple CLI-based coding agent in Python that can read files, write content, edit text, and execute shell commands. Don't worry if you're new to AI agents - this tutorial is designed to be approachable and practical. You'll be amazed at how much you can accomplish with surprisingly little code.\n",
    "\n",
    "The diagram below illustrates the fundamental agentic loop in action:\n",
    "\n",
    " ```mermaid\n",
    " graph TD\n",
    "     Start[User Input] --> Loop[LLM Inference]\n",
    "\n",
    "     Loop -->|generates| Output[LLM Output]\n",
    "\n",
    "     Output --> EndsCheck{Ends?}\n",
    "\n",
    "     EndsCheck -->|Yes| Exit[Break/Exit Loop]\n",
    "\n",
    "     EndsCheck -->|No| ToolCheck{Tool Calls?}\n",
    "\n",
    "     ToolCheck -->|Yes| Tools[Execute Tool Calls]\n",
    "     Tools -->|tool results as new user input| Loop\n",
    "\n",
    "     ToolCheck -->|No| Loop\n",
    "\n",
    "     style EndsCheck fill:#faa,stroke:#333,stroke-width:2px\n",
    "     style ToolCheck fill:#f9f,stroke:#333,stroke-width:2px\n",
    "     style Tools fill:#bfb,stroke:#333,stroke-width:2px\n",
    "     style Exit fill:#ddd,stroke:#333,stroke-width:2px\n",
    " ```\n",
    "\n",
    "By the end of this tutorial, we'll have a Python script that's just over a handred lines of code - compact, powerful, and surprisingly capable of giving you the 'Oh look at that!' dopamine hits. I've structured this to be easy to follow along step by step, and I highly encourage you to try it out yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "You need an Anthropic API key for accessing to Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "if not api_key:\n",
    "    api_key = getpass.getpass(\"Enter your Anthropic API key: \")\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = api_key# Your agent code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Core Loop\n",
    "\n",
    "Let's start with the foundation. We will create an agent called HAL. Every AI agent needs a chat loop that maintains context and handles the back-and-forth between user and AI. Here's our basic structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import AsyncAnthropic\n",
    "import json, os, asyncio\n",
    "from datetime import datetime\n",
    "\n",
    "client = AsyncAnthropic()\n",
    "\n",
    "async def loop(system_prompt, messages, user_input):\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    msg = await client.messages.create(\n",
    "        max_tokens=2048,\n",
    "        thinking={\"type\": \"enabled\", \"budget_tokens\": 1024},\n",
    "        system=[{\"type\": \"text\", \"text\": system_prompt}],\n",
    "        messages=messages,\n",
    "        model=\"claude-sonnet-4-5\"\n",
    "    )\n",
    "\n",
    "    messages.append({\"role\": \"assistant\", \"content\": msg.content})\n",
    "\n",
    "    thinking_text = ' '.join(t.thinking for t in msg.content if t.type == 'thinking')\n",
    "    if thinking_text:\n",
    "        print(f\"ðŸ’­ {thinking_text}\")\n",
    "\n",
    "    agent_text = ' '.join(t.text for t in msg.content if t.type == 'text')\n",
    "    if agent_text:\n",
    "        print(f\"ðŸ¤– {agent_text}\")\n",
    "\n",
    "async def main():\n",
    "    messages = []\n",
    "    system_prompt = f\"\"\"Your name is HAL.\n",
    "You are an interactive CLI tool that helps with software engineering and production operations tasks.\n",
    "You are running on {str(os.uname())}, today is {datetime.now().strftime('%Y-%m-%d')}\n",
    "\"\"\"\n",
    "    print(\"enter 'exit' to quit\")\n",
    "    while True:\n",
    "        user_input = input(\"> \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"ðŸ‘‹ Goodbye!\")\n",
    "            break\n",
    "        await loop(system_prompt, messages, user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter 'exit' to quit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‹ Goodbye!\n"
     ]
    }
   ],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make sure you type exit in the end otherwise the rest of the cell cannot be executed!**\n",
    "\n",
    "This script establishes the core conversation loop of our agent. The loop function handles the core conversation cycle: it adds user input to the message history, sends everything to Claude, processes the response, and displays it. The thinking parameter enables Claude's extended thinking and reasoning capabilities with a 1024-token budget, allowing it to work through complex problems step-by-step thought process. The main function creates an interactive REPL that maintains conversation context across exchanges.\n",
    "\n",
    "This gives us a working chat interface where we can have fascinating conversations with HAL, after all it embodies vast human knowledge from across the internet. However, HAL remains confined to the realm of text and imagesâ€”the only way for it to interact with the outer world is through you manually feeding it information such as code, logs, compiler errors, etc., which is not only inefficient but also potentially biased. To bridge this gap and give it the ability to act in the real world, we need to equip it with tools so that it can read files, run programs to get results and errors autonomously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Tool Capabilities\n",
    "\n",
    "As for large language models, the most straightforward way to feed them with external information is to provide them the ability to read text, therefore the first tool we'll implement is file reading. The function below shows file reading in its simplest form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiofiles\n",
    "from typing import Annotated\n",
    "\n",
    "async def read_file(filename: Annotated[str, \"The path to the file to read\"]):\n",
    "    \"\"\"Read the whole file\"\"\"\n",
    "    async with aiofiles.open(filename, 'r') as f:\n",
    "        return {\"success\": True, \"filename\": filename, \"output\": await f.read()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function works, but HAL doesn't know its existence, neither does it have access to the Python runtime. We need to bridge the gap between Python functions and jsonschemas that the model can understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tool System\n",
    "\n",
    "Here's where the magic happens. We'll create a decorator that automatically generates the necessary schemas and a toolbox to manage everything:\n",
    "\n",
    "**Don't worry if you can't understand what this function does. Just use it as a utility**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, get_type_hints, get_origin, get_args\n",
    "from inspect import Parameter, signature\n",
    "from pydantic import create_model, Field\n",
    "\n",
    "\n",
    "def tool(f):\n",
    "    def _process_parameter(name: str, param: Parameter, hints: dict) -> tuple:\n",
    "        \"\"\"Process a single parameter into a model field specification.\"\"\"\n",
    "        default = ... if param.default == Parameter.empty else param.default\n",
    "        hint = hints.get(name, param.annotation)\n",
    "\n",
    "        if get_origin(hint) == Annotated:\n",
    "            base_type, *metadata = get_args(hint)\n",
    "            description = next((m for m in metadata if isinstance(m, str)), None)\n",
    "            return (base_type, Field(default=default, description=description) if description else default)\n",
    "\n",
    "        return (hint, default)\n",
    "\n",
    "    hints = get_type_hints(f, include_extras=True)\n",
    "    model_fields = { name: _process_parameter(name, param, hints) for name, param in signature(f).parameters.items() }\n",
    "\n",
    "    m = create_model(f'{f.__name__} Input', **model_fields)\n",
    "    m.run = lambda self: f(**self.model_dump())\n",
    "\n",
    "    return {\n",
    "        \"name\": f.__name__,\n",
    "        \"description\": f.__doc__ or f\"Tool: {f.__name__}\",\n",
    "        \"model\": m\n",
    "    }\n",
    "\n",
    "class Toolbox:\n",
    "    def __init__(self, tools):\n",
    "        self.tools = tools\n",
    "\n",
    "    def schema(self):\n",
    "        return [{\n",
    "            \"name\": t[\"name\"],\n",
    "            \"description\": t[\"description\"],\n",
    "            \"input_schema\": t[\"model\"].model_json_schema()\n",
    "        } for t in self.tools]\n",
    "\n",
    "    async def run(self, name, input):\n",
    "        tool = next(t for t in self.tools if t[\"name\"] == name)\n",
    "        return await tool[\"model\"](**input).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At its core, this code creates a bridge between Python functions and AI models. The @tool decorator examines function's signature and automatically generates a JSON schema that describes what inputs the function expects - think of it as creating a user manual for the Python function that the AI can read. The Toolbox class acts like a registry, collecting all these tool schemas and providing a way to execute them when the AI requests it. When HAL wants to use a tool, it sends a structured output that matches the schema, and the toolbox validates the input and runs the corresponding Python function.\n",
    "\n",
    "Now we can decorate our file reading function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "async def read_file(filename: Annotated[str, \"The path to the file to read\"]):\n",
    "    \"\"\"Read the whole file\"\"\"\n",
    "    async with aiofiles.open(filename, 'r') as f:\n",
    "        return {\"success\": True, \"filename\": filename, \"output\": await f.read()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once decorated, we can generate a JSON schema that describes the function to the AI model. Each schema at minimum contains the function's name, description, and parameter specifications. This schema acts as a contract when HAL wants to use a tool, it must structure its output according to this schema, making it the protocol between the HAL and the Python runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"name\": \"read_file\",\n",
      "    \"description\": \"Read the whole file\",\n",
      "    \"input_schema\": {\n",
      "      \"properties\": {\n",
      "        \"filename\": {\n",
      "          \"description\": \"The path to the file to read\",\n",
      "          \"title\": \"Filename\",\n",
      "          \"type\": \"string\"\n",
      "        }\n",
      "      },\n",
      "      \"required\": [\n",
      "        \"filename\"\n",
      "      ],\n",
      "      \"title\": \"read_file Input\",\n",
      "      \"type\": \"object\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "toolbox = Toolbox([read_file])\n",
    "print(json.dumps(toolbox.schema(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting Tools to the Loop\n",
    "Now we need to update our main loop to handle tool calls. When Claude wants to use a tool, it returns special tool_use blocks instead of just text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def loop(system_prompt, toolbox, messages, user_input):\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    while True:\n",
    "        msg = await client.messages.create(\n",
    "            max_tokens=2048,\n",
    "            thinking={\"type\": \"enabled\", \"budget_tokens\": 1024},\n",
    "            system=[{\"type\": \"text\", \"text\": system_prompt}],\n",
    "            messages=messages,\n",
    "            model=\"claude-sonnet-4-5\",\n",
    "            tools=toolbox.schema()\n",
    "        )\n",
    "\n",
    "        messages.append({\"role\": \"assistant\", \"content\": msg.content})\n",
    "\n",
    "        thinking_text = ' '.join(t.thinking for t in msg.content if t.type == 'thinking')\n",
    "        if thinking_text:\n",
    "            print(f\"ðŸ’­ {thinking_text}\")\n",
    "\n",
    "        agent_text = ' '.join(t.text for t in msg.content if t.type == 'text')\n",
    "        if agent_text:\n",
    "            print(f\"ðŸ¤– {agent_text}\")\n",
    "\n",
    "        tools = [t for t in msg.content if t.type == \"tool_use\"]\n",
    "        if not tools:\n",
    "            break\n",
    "\n",
    "        # Execute all tools and collect results\n",
    "        results = await asyncio.gather(*[\n",
    "            toolbox.run(t.name, t.input) for t in tools\n",
    "        ])\n",
    "\n",
    "        # Display results and send back to model\n",
    "        for t, r in zip(tools, results):\n",
    "            status = \"âœ…\" if r.get(\"success\") else \"âŒ\"\n",
    "            print(f\"{status} {t.name}:\")\n",
    "            print(json.dumps(r, indent=2))\n",
    "\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\n",
    "                \"type\": \"tool_result\",\n",
    "                \"tool_use_id\": t.id,\n",
    "                \"content\": json.dumps(r)\n",
    "            } for t, r in zip(tools, results)]\n",
    "        })\n",
    "\n",
    "async def run_agent(tools=[]):\n",
    "    toolbox = Toolbox(tools)\n",
    "    messages = []\n",
    "    system_prompt = f\"\"\"Your name is HAL.\n",
    "You are an interactive CLI tool that helps with software engineering and production operations tasks.\n",
    "You are running on {str(os.uname())}, today is {datetime.now().strftime('%Y-%m-%d')}\n",
    "\"\"\"\n",
    "    print(\"enter 'exit' to quit\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            user_input = input(\"> \")\n",
    "            if user_input.lower() == \"exit\":\n",
    "                print(\"ðŸ‘‹ Goodbye!\")\n",
    "                break\n",
    "            await loop(system_prompt, toolbox, messages, user_input)\n",
    "    except (KeyboardInterrupt, EOFError):\n",
    "        print(\"ðŸ‘‹ Goodbye!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter 'exit' to quit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‹ Goodbye!\n"
     ]
    }
   ],
   "source": [
    "await run_agent(tools=[read_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding More Tools\n",
    "With our infrastructure in place, adding more capabilities becomes straightforward. Let's implement three essential tools: file writing, editing, and shell execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "async def write_file(filename: str, content: str):\n",
    "    \"\"\"Write the file with the content. This is an overwrite\"\"\"\n",
    "    async with aiofiles.open(filename, 'w') as f:\n",
    "        await f.write(content)\n",
    "        return {\"success\": True, \"filename\": filename, \"output\": f\"wrote to {filename}\"}\n",
    "\n",
    "@tool\n",
    "async def edit_file(filename: str, old_text: Annotated[str, \"the text to replace\"], new_text: Annotated[str, \"the text to replace with\"]):\n",
    "    \"\"\"Edit the file with the new text. Note that the text to replace should only appear once in the file.\"\"\"\n",
    "    async with aiofiles.open(filename, 'r') as f:\n",
    "        content = await f.read()\n",
    "\n",
    "    if content.count(old_text) != 1:\n",
    "        return {\"success\": False, \"filename\": filename, \"output\": f\"old text appears {content.count(old_text)} times in the file\"}\n",
    "\n",
    "    async with aiofiles.open(filename, 'w') as f:\n",
    "        await f.write(content.replace(old_text, new_text))\n",
    "\n",
    "    return {\"success\": True, \"filename\": filename, \"output\": f\"edited {filename}\"}\n",
    "\n",
    "@tool\n",
    "async def shell(command: Annotated[str, \"command to execute\"], timeout: Annotated[int, \"timeout in seconds\"] = 30):\n",
    "    \"\"\"Execute a bash command\"\"\"\n",
    "    try:\n",
    "        p = await asyncio.create_subprocess_shell(\n",
    "            command,\n",
    "            stdout=asyncio.subprocess.PIPE,\n",
    "            stderr=asyncio.subprocess.STDOUT\n",
    "        )\n",
    "        stdout, _ = await asyncio.wait_for(p.communicate(), timeout)\n",
    "        return {\"success\": p.returncode == 0, \"command\": command, \"output\": stdout.decode()}\n",
    "    except asyncio.TimeoutError:\n",
    "        return {\"success\": False, \"command\": command, \"output\": \"Timeout\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together\n",
    "Finally, we wire everything up in our main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter 'exit' to quit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  what file can you read on this system?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’­ The user is asking what files I can read on this system. I should help them understand what's available. Let me start by checking what's in the current directory and maybe some common locations.\n",
      "\n",
      "I'll use the shell command to list files in the current directory first.\n",
      "ðŸ¤– I can read any files that are accessible on this system. Let me show you what's in the current directory:\n",
      "âœ… shell:\n",
      "{\n",
      "  \"success\": true,\n",
      "  \"command\": \"pwd && ls -la\",\n",
      "  \"output\": \"/home/jingkaihe/workspace/llm-agent-from-scratch\\ntotal 256\\ndrwxrwxr-x  6 jingkaihe jingkaihe   4096 Nov  2 11:02 .\\ndrwxr-xr-x 49 jingkaihe jingkaihe   4096 Nov  2 08:50 ..\\ndrwxrwxr-x  8 jingkaihe jingkaihe   4096 Nov  2 10:24 .git\\n-rw-rw-r--  1 jingkaihe jingkaihe    237 Nov  2 10:20 .gitignore\\ndrwxrwxr-x  2 jingkaihe jingkaihe   4096 Nov  2 10:52 .ipynb_checkpoints\\ndrwxr-xr-x  3 jingkaihe jingkaihe   4096 Nov  2 08:59 .kodelet\\n-rw-rw-r--  1 jingkaihe jingkaihe      5 Nov  2 08:51 .python-version\\ndrwxrwxr-x  6 jingkaihe jingkaihe   4096 Nov  2 08:51 .venv\\n-rw-rw-r--  1 jingkaihe jingkaihe   1251 Nov  2 10:23 README.md\\n-rw-rw-r--  1 jingkaihe jingkaihe    998 Nov  2 10:56 Untitled.ipynb\\n-rw-r--r--  1 jingkaihe jingkaihe  27040 Nov  2 11:02 llm-agent-from-scratch.ipynb\\n-rw-rw-r--  1 jingkaihe jingkaihe   6282 Nov  2 10:19 main.py\\n-rw-rw-r--  1 jingkaihe jingkaihe    264 Nov  2 09:48 pyproject.toml\\n-rw-rw-r--  1 jingkaihe jingkaihe 176970 Nov  2 09:48 uv.lock\\n\"\n",
      "}\n",
      "ðŸ¤– Great! I'm currently in `/home/jingkaihe/workspace/llm-agent-from-scratch`. I can read any files on this system that are accessible with the current user permissions. \n",
      "\n",
      "In this directory, I can read:\n",
      "- **README.md** - project documentation\n",
      "- **main.py** - Python source code\n",
      "- **pyproject.toml** - Python project configuration\n",
      "- **llm-agent-from-scratch.ipynb** - Jupyter notebook\n",
      "- **Untitled.ipynb** - Another Jupyter notebook\n",
      "- **.gitignore** - Git configuration\n",
      "- **.python-version** - Python version file\n",
      "- **uv.lock** - UV package lock file\n",
      "\n",
      "Would you like me to read any of these files? Just let me know which one interests you, or I can also navigate to other directories if you need to access files elsewhere on the system.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  can you read main.py and summarise what it does?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’­ The user wants me to read main.py and provide a summary of what it does.\n",
      "âœ… read_file:\n",
      "{\n",
      "  \"success\": true,\n",
      "  \"filename\": \"main.py\",\n",
      "  \"output\": \"from typing import Annotated, get_type_hints, get_origin, get_args\\nfrom inspect import Parameter, signature\\nfrom pydantic import create_model, Field\\nimport aiofiles\\nfrom anthropic import AsyncAnthropic\\nimport json, os, asyncio\\nfrom datetime import datetime\\n\\n\\ndef tool(f):\\n    def _process_parameter(name: str, param: Parameter, hints: dict) -> tuple:\\n        \\\"\\\"\\\"Process a single parameter into a model field specification.\\\"\\\"\\\"\\n        default = ... if param.default == Parameter.empty else param.default\\n        hint = hints.get(name, param.annotation)\\n\\n        if get_origin(hint) == Annotated:\\n            base_type, *metadata = get_args(hint)\\n            description = next((m for m in metadata if isinstance(m, str)), None)\\n            return (\\n                base_type,\\n                Field(default=default, description=description)\\n                if description\\n                else default,\\n            )\\n\\n        return (hint, default)\\n\\n    hints = get_type_hints(f, include_extras=True)\\n    model_fields = {\\n        name: _process_parameter(name, param, hints)\\n        for name, param in signature(f).parameters.items()\\n    }\\n\\n    m = create_model(f\\\"{f.__name__} Input\\\", **model_fields)\\n    m.run = lambda self: f(**self.model_dump())\\n\\n    return {\\n        \\\"name\\\": f.__name__,\\n        \\\"description\\\": f.__doc__ or f\\\"Tool: {f.__name__}\\\",\\n        \\\"model\\\": m,\\n    }\\n\\n\\nclass Toolbox:\\n    def __init__(self, tools):\\n        self.tools = tools\\n\\n    def schema(self):\\n        return [\\n            {\\n                \\\"name\\\": t[\\\"name\\\"],\\n                \\\"description\\\": t[\\\"description\\\"],\\n                \\\"input_schema\\\": t[\\\"model\\\"].model_json_schema(),\\n            }\\n            for t in self.tools\\n        ]\\n\\n    async def run(self, name, input):\\n        tool = next(t for t in self.tools if t[\\\"name\\\"] == name)\\n        return await tool[\\\"model\\\"](**input).run()\\n\\n\\n@tool\\nasync def read_file(filename: Annotated[str, \\\"The path to the file to read\\\"]):\\n    \\\"\\\"\\\"Read the whole file\\\"\\\"\\\"\\n    async with aiofiles.open(filename, \\\"r\\\") as f:\\n        return {\\\"success\\\": True, \\\"filename\\\": filename, \\\"output\\\": await f.read()}\\n\\n\\n@tool\\nasync def write_file(filename: str, content: str):\\n    \\\"\\\"\\\"Write the file with the content. This is an overwrite\\\"\\\"\\\"\\n    async with aiofiles.open(filename, \\\"w\\\") as f:\\n        await f.write(content)\\n        return {\\\"success\\\": True, \\\"filename\\\": filename, \\\"output\\\": f\\\"wrote to {filename}\\\"}\\n\\n\\n@tool\\nasync def edit_file(\\n    filename: str,\\n    old_text: Annotated[str, \\\"the text to replace\\\"],\\n    new_text: Annotated[str, \\\"the text to replace with\\\"],\\n):\\n    \\\"\\\"\\\"Edit the file with the new text. Note that the text to replace should only appear once in the file.\\\"\\\"\\\"\\n    async with aiofiles.open(filename, \\\"r\\\") as f:\\n        content = await f.read()\\n\\n    if content.count(old_text) != 1:\\n        return {\\n            \\\"success\\\": False,\\n            \\\"filename\\\": filename,\\n            \\\"output\\\": f\\\"old text appears {content.count(old_text)} times in the file\\\",\\n        }\\n\\n    async with aiofiles.open(filename, \\\"w\\\") as f:\\n        await f.write(content.replace(old_text, new_text))\\n\\n    return {\\\"success\\\": True, \\\"filename\\\": filename, \\\"output\\\": f\\\"edited {filename}\\\"}\\n\\n\\n@tool\\nasync def shell(\\n    command: Annotated[str, \\\"command to execute\\\"],\\n    timeout: Annotated[int, \\\"timeout in seconds\\\"] = 30,\\n):\\n    \\\"\\\"\\\"Execute a bash command\\\"\\\"\\\"\\n    try:\\n        p = await asyncio.create_subprocess_shell(\\n            command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.STDOUT\\n        )\\n        stdout, _ = await asyncio.wait_for(p.communicate(), timeout)\\n        return {\\n            \\\"success\\\": p.returncode == 0,\\n            \\\"command\\\": command,\\n            \\\"output\\\": stdout.decode(),\\n        }\\n    except asyncio.TimeoutError:\\n        return {\\\"success\\\": False, \\\"command\\\": command, \\\"output\\\": \\\"Timeout\\\"}\\n\\n\\nasync def loop(system_prompt, toolbox, messages, user_input):\\n    messages.append({\\\"role\\\": \\\"user\\\", \\\"content\\\": user_input})\\n\\n    while True:\\n        msg = await client.messages.create(\\n            max_tokens=2048,\\n            thinking={\\\"type\\\": \\\"enabled\\\", \\\"budget_tokens\\\": 1024},\\n            system=[{\\\"type\\\": \\\"text\\\", \\\"text\\\": system_prompt}],\\n            messages=messages,\\n            model=\\\"claude-sonnet-4-5\\\",\\n            tools=toolbox.schema(),\\n        )\\n\\n        messages.append({\\\"role\\\": \\\"assistant\\\", \\\"content\\\": msg.content})\\n\\n        thinking_text = \\\" \\\".join(\\n            t.thinking for t in msg.content if t.type == \\\"thinking\\\"\\n        )\\n        if thinking_text:\\n            print(f\\\"\\ud83d\\udcad {thinking_text}\\\")\\n\\n        agent_text = \\\" \\\".join(t.text for t in msg.content if t.type == \\\"text\\\")\\n        if agent_text:\\n            print(f\\\"\\ud83e\\udd16 {agent_text}\\\")\\n\\n        tools = [t for t in msg.content if t.type == \\\"tool_use\\\"]\\n        if not tools:\\n            break\\n\\n        # Execute all tools and collect results\\n        results = await asyncio.gather(*[toolbox.run(t.name, t.input) for t in tools])\\n\\n        # Display results and send back to model\\n        for t, r in zip(tools, results):\\n            status = \\\"\\u2705\\\" if r.get(\\\"success\\\") else \\\"\\u274c\\\"\\n            print(f\\\"{status} {t.name}:\\\")\\n            print(json.dumps(r, indent=2))\\n\\n        messages.append(\\n            {\\n                \\\"role\\\": \\\"user\\\",\\n                \\\"content\\\": [\\n                    {\\n                        \\\"type\\\": \\\"tool_result\\\",\\n                        \\\"tool_use_id\\\": t.id,\\n                        \\\"content\\\": json.dumps(r),\\n                    }\\n                    for t, r in zip(tools, results)\\n                ],\\n            }\\n        )\\n\\n\\nclient = AsyncAnthropic()\\n\\n\\nasync def run_agent(tools=[]):\\n    toolbox = Toolbox(tools)\\n    messages = []\\n    system_prompt = f\\\"\\\"\\\"Your name is HAL.\\nYou are an interactive CLI tool that helps with software engineering and production operations tasks.\\nYou are running on {str(os.uname())}, today is {datetime.now().strftime(\\\"%Y-%m-%d\\\")}\\n\\\"\\\"\\\"\\n    print(\\\"enter 'exit' to quit\\\")\\n\\n    try:\\n        while True:\\n            user_input = input(\\\"> \\\")\\n            if user_input.lower() == \\\"exit\\\":\\n                print(\\\"\\ud83d\\udc4b Goodbye!\\\")\\n                break\\n            await loop(system_prompt, toolbox, messages, user_input)\\n    except (KeyboardInterrupt, EOFError):\\n        print(\\\"\\ud83d\\udc4b Goodbye!\\\")\\n\\n\\nasyncio.run(run_agent(tools=[read_file, write_file, edit_file, shell]))\\n\"\n",
      "}\n",
      "ðŸ¤– ## Summary of main.py\n",
      "\n",
      "This Python script implements **an LLM-powered agent system** that can interact with your computer through tools. Here's what it does:\n",
      "\n",
      "### Core Components:\n",
      "\n",
      "1. **`@tool` decorator**: Converts Python functions into tool definitions that an LLM can use. It automatically:\n",
      "   - Extracts type hints and creates Pydantic models for parameter validation\n",
      "   - Handles `Annotated` types to extract parameter descriptions\n",
      "   - Generates JSON schemas for the API\n",
      "\n",
      "2. **`Toolbox` class**: Manages a collection of tools:\n",
      "   - Converts tools to schemas the LLM can understand\n",
      "   - Executes tools by name with given inputs\n",
      "\n",
      "3. **Four Built-in Tools**:\n",
      "   - `read_file`: Read contents of a file\n",
      "   - `write_file`: Write/overwrite a file with content\n",
      "   - `edit_file`: Replace text in a file (must appear exactly once)\n",
      "   - `shell`: Execute bash commands with timeout\n",
      "\n",
      "4. **Agent Loop (`loop` function)**:\n",
      "   - Sends user input to Claude (Sonnet 4.5) with extended thinking enabled\n",
      "   - Processes Claude's responses including thinking blocks\n",
      "   - Executes any tools Claude requests\n",
      "   - Returns results back to Claude for further processing\n",
      "   - Continues until Claude doesn't request more tools\n",
      "\n",
      "5. **Main Interface (`run_agent`)**:\n",
      "   - Creates an interactive CLI where you can chat with \"HAL\"\n",
      "   - Provides system context (OS info, date)\n",
      "   - Maintains conversation history\n",
      "   - Handles graceful exit\n",
      "\n",
      "Essentially, this is **a self-hosted AI agent** similar to what I am! It uses Anthropic's Claude API to create an assistant that can read/write files and execute shell commands autonomously.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  What is the uv.lock file?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’­ The user is asking about the uv.lock file. This is a lock file created by uv, which is a fast Python package manager written in Rust. It's similar to poetry.lock or requirements.lock files. I could read it to show them what it contains, but it's likely quite large (the ls output showed it's 176970 bytes, so about 177 KB). \n",
      "\n",
      "Let me explain what it is and offer to show them parts of it if they want.\n",
      "ðŸ¤– The `uv.lock` file is a **lock file** created by [uv](https://github.com/astral-sh/uv), which is a fast Python package manager and installer written in Rust (by the same team that makes Ruff).\n",
      "\n",
      "It serves a similar purpose to:\n",
      "- `poetry.lock` (for Poetry)\n",
      "- `Pipfile.lock` (for Pipenv)\n",
      "- `package-lock.json` (for npm)\n",
      "\n",
      "### What it does:\n",
      "- **Locks exact versions** of all dependencies (direct and transitive) used in the project\n",
      "- Ensures **reproducible builds** - anyone installing the project gets the exact same package versions\n",
      "- Contains hashes for package integrity verification\n",
      "- Records the dependency resolution at a specific point in time\n",
      "\n",
      "Looking at your project, you have both `pyproject.toml` (which declares your dependencies) and `uv.lock` (which locks the exact versions resolved).\n",
      "\n",
      "Would you like me to read parts of it to show you what's inside? It's about 177 KB, so it's quite large. I could also check the pyproject.toml to see what the project's direct dependencies are, which would be more readable.\n"
     ]
    }
   ],
   "source": [
    "await run_agent(tools=[read_file, write_file, edit_file, shell])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What many people don't realise is that frontier models like Claude possess incredible system operations knowledge. Once you provide it with shell access and basic system information, even without extensive prompting, these models demonstrate expert-level operations capabilities. Claude Sonnet models in particular exhibit **persistent, relentless and infectiously optimistic** problem-solving behavior. When asked a question like \"how is python3 installed on this machine?\", they will systematically explore the filesystem, examine package managers, check environment variables, and execute diagnostic commands until they find a definitive answer.\n",
    "\n",
    "When commands fail or produce errors, the model treats these as valuable debugging signals, much like an experienced engineer would. It analyses error messages, adjusts its approach, and tries alternative strategies to work around the problem.\n",
    "\n",
    "While hallucination remains a concern (perhaps one of the most common criticisms of LLMs), the model's ability to execute commands creates a unreasonably effective feedback loop. When its assumptions prove incorrect, the resulting errors quickly ground the model back in reality, forcing it to revise its mental model and try new approaches based on actual system behavior rather than potentially outdated training data or hallucinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCP MCP MCP (And Why You Might not Want It)\n",
    "\n",
    "MCP (Model Context Protocol) is an open source industry standard for connecting AI applications to external systems. Considering it as a standardrised tool box interface that we have just implemented\n",
    "\n",
    "> Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect electronic devices, MCP provides a standardized way to connect AI applications to external systems.\n",
    ">\n",
    "> -- https://modelcontextprotocol.io/docs/getting-started/intro\n",
    "\n",
    "Integrate MCP into the existing HAL bot is, in fact fairly trivial because under the hood the tool call is also implemented in JSON schema.\n",
    "\n",
    "### MCP Architecture\n",
    "\n",
    "The diagram below illustrates how MCP integration works in our agent. MCP servers run as separate processes, communicating with the HAL agent through standard input/output (stdio) using JSON-RPC protocol:\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    subgraph \"HAL Agent Process\"\n",
    "        Agent[HAL Agent]\n",
    "        Toolbox[Toolbox Manager]\n",
    "        LocalTools[Local Tools<br/>read_file, write_file<br/>edit_file, shell]\n",
    "        \n",
    "        Agent -->|uses| Toolbox\n",
    "        Toolbox -->|manages| LocalTools\n",
    "    end\n",
    "    \n",
    "    subgraph \"MCP Server 1 Process\"\n",
    "        MCP1[MCP Server<br/>@modelcontextprotocol/server-filesystem]\n",
    "        Tools1[Tools:<br/>read_file, write_file<br/>create_directory, list_directory]\n",
    "        MCP1 -.->|exposes| Tools1\n",
    "    end\n",
    "    \n",
    "    subgraph \"MCP Server N Process\"\n",
    "        MCPN[MCP Server<br/>custom-server]\n",
    "        ToolsN[Tools:<br/>custom_tool_1<br/>custom_tool_2]\n",
    "        MCPN -.->|exposes| ToolsN\n",
    "    end\n",
    "\n",
    "    Claude[Anthropic API] <-->|\"Tool schemas +<br/>Tool execution requests\"| Agent\n",
    "    Toolbox <-->|\"JSON-RPC over stdio<br/>(initialize, list_tools, call_tool)\"| MCP1\n",
    "    Toolbox <-->|\"JSON-RPC over stdio<br/>(initialize, list_tools, call_tool)\"| MCPN\n",
    "    \n",
    "    \n",
    "    style Agent fill:#e1f5ff,stroke:#01579b,stroke-width:3px\n",
    "    style Toolbox fill:#fff9c4,stroke:#f57f17,stroke-width:2px\n",
    "    style MCP1 fill:#f3e5f5,stroke:#4a148c,stroke-width:2px\n",
    "    style MCPN fill:#f3e5f5,stroke:#4a148c,stroke-width:2px\n",
    "    style Claude fill:#c8e6c9,stroke:#1b5e20,stroke-width:2px\n",
    "```\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Process Spawning**: When the Toolbox is initialized, it spawns each MCP server as a separate subprocess using the configuration from `mcp.yaml`. Each server runs with its own command (e.g., `npx`, `uvx`, `docker`).\n",
    "\n",
    "2. **JSON-RPC Communication**: The HAL agent communicates with MCP servers through their standard input/output streams using JSON-RPC 2.0 protocol:\n",
    "   - `initialize`: Handshake to establish connection\n",
    "   - `list_tools`: Discover what tools the server provides\n",
    "   - `call_tool`: Execute a specific tool with parameters\n",
    "\n",
    "3. **Tool Discovery**: During startup, the Toolbox queries each MCP server to get its available tools. These tools are combined with local tools into a unified schema that LLM can use.\n",
    "\n",
    "4. **Tool Execution**: When Claude requests a tool execution:\n",
    "   - The Toolbox checks if it's a local tool or MCP tool\n",
    "   - For local tools: Executes the Python function directly\n",
    "   - For MCP tools: Sends a JSON-RPC `call_tool` request to the appropriate MCP server and waits for the response\n",
    "\n",
    "5. **Lifecycle Management**: The Toolbox uses async context managers to properly:\n",
    "   - Establish mcp servers connections on agent start up \n",
    "   - Clean up and close all MCP server processes when exiting\n",
    "\n",
    "Here is our revised version of toolbox to support MCP integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from getpass import getpass\n",
    "import yaml\n",
    "\n",
    "class Toolbox:\n",
    "    \"\"\"Manages both local and MCP tools with proper async lifecycle management\"\"\"\n",
    "\n",
    "    def __init__(self, local_tools=[], mcp_servers=[]):\n",
    "        self.local_tools = local_tools\n",
    "        self.mcp_servers = mcp_servers\n",
    "        self.mcp_tools = []\n",
    "        self.mcp_connections = []\n",
    "\n",
    "    async def __aenter__(self):\n",
    "        \"\"\"Async context manager entry - connect to MCP servers\"\"\"\n",
    "        if self.mcp_servers:\n",
    "            await self._connect_mcp_servers()\n",
    "        return self\n",
    "\n",
    "    async def __aexit__(self, exc_type, exc_val, exc_tb):\n",
    "        \"\"\"Async context manager exit - cleanup MCP connections\"\"\"\n",
    "        await self.cleanup()\n",
    "        return False\n",
    "\n",
    "    async def _connect_mcp_servers(self):\n",
    "        \"\"\"Connect to all MCP servers and collect their tools\"\"\"\n",
    "        for server_config in self.mcp_servers:\n",
    "            # Expand cwd if it's a relative path or ~\n",
    "            cwd = server_config.get(\"cwd\")\n",
    "            if cwd:\n",
    "                cwd = str(Path(cwd).expanduser().resolve())\n",
    "\n",
    "            for env_var in server_config.get(\"env\", {}).keys():\n",
    "                value = server_config[\"env\"].get(env_var)\n",
    "                if value is None:\n",
    "                    value = os.environ.pop(env_var, None)\n",
    "                if value is None:\n",
    "                    value = getpass(f\"Enter value for {env_var}: \")\n",
    "                    value = value.strip()\n",
    "                server_config[\"env\"][env_var] = value\n",
    "\n",
    "            server_params = StdioServerParameters(\n",
    "                command=server_config[\"command\"],\n",
    "                args=server_config.get(\"args\", []),\n",
    "                env=server_config.get(\"env\"),\n",
    "                cwd=cwd,\n",
    "            )\n",
    "\n",
    "            # Create and enter context managers\n",
    "            stdio_ctx = stdio_client(server_params)\n",
    "            read, write = await stdio_ctx.__aenter__()\n",
    "\n",
    "            session_ctx = ClientSession(read, write)\n",
    "            session = await session_ctx.__aenter__()\n",
    "            await session.initialize()\n",
    "\n",
    "            # Store contexts for cleanup\n",
    "            self.mcp_connections.append(\n",
    "                {\n",
    "                    \"stdio_ctx\": stdio_ctx,\n",
    "                    \"session_ctx\": session_ctx,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Get tools from this server\n",
    "            tools_list = await session.list_tools()\n",
    "\n",
    "            for tool in tools_list.tools:\n",
    "                self.mcp_tools.append(\n",
    "                    {\n",
    "                        \"name\": tool.name,\n",
    "                        \"description\": tool.description or f\"MCP Tool: {tool.name}\",\n",
    "                        \"input_schema\": tool.inputSchema,\n",
    "                        \"mcp_session\": session,\n",
    "                        \"type\": \"mcp\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    async def cleanup(self):\n",
    "        \"\"\"Properly cleanup all MCP connections\"\"\"\n",
    "        for conn in reversed(self.mcp_connections):\n",
    "            try:\n",
    "                await conn[\"session_ctx\"].__aexit__(None, None, None)\n",
    "            except Exception as e:\n",
    "                print(f\"Error closing session: {e}\")\n",
    "\n",
    "            try:\n",
    "                await conn[\"stdio_ctx\"].__aexit__(None, None, None)\n",
    "            except Exception as e:\n",
    "                print(f\"Error closing stdio: {e}\")\n",
    "\n",
    "    @property\n",
    "    def all_tools(self):\n",
    "        \"\"\"Return all tools (local + MCP)\"\"\"\n",
    "        return self.local_tools + self.mcp_tools\n",
    "\n",
    "    def schema(self):\n",
    "        return [\n",
    "            {\n",
    "                \"name\": t[\"name\"],\n",
    "                \"description\": t[\"description\"],\n",
    "                \"input_schema\": t.get(\"input_schema\") or t[\"model\"].model_json_schema(),\n",
    "            }\n",
    "            for t in self.all_tools\n",
    "        ]\n",
    "\n",
    "    async def run(self, name, input):\n",
    "        tool = next(t for t in self.all_tools if t[\"name\"] == name)\n",
    "\n",
    "        if tool.get(\"type\") == \"mcp\":\n",
    "            result = await tool[\"mcp_session\"].call_tool(name, input)\n",
    "            return {\n",
    "                \"success\": not result.isError,\n",
    "                \"output\": \"\\n\".join(\n",
    "                    c.text for c in result.content if hasattr(c, \"text\")\n",
    "                ),\n",
    "            }\n",
    "        else:\n",
    "            return await tool[\"model\"](**input).run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And revised run_agent core loop to support graceful MCP startup and shutdown via Python's context lifecycle management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mcp_config(config_path=\"mcp.yaml\"):\n",
    "    \"\"\"Load MCP server configuration from YAML file\"\"\"\n",
    "    config_file = Path(config_path)\n",
    "\n",
    "    if not config_file.exists():\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        with open(config_file, \"r\") as f:\n",
    "            config = yaml.safe_load(f)\n",
    "            return config.get(\"servers\", [])\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Error loading MCP config from {config_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "async def run_agent(tools=[], mcp_servers=[]):\n",
    "    \"\"\"Run the agent with local tools and optional MCP servers\"\"\"\n",
    "    async with Toolbox(local_tools=tools, mcp_servers=mcp_servers) as toolbox:\n",
    "        messages = []\n",
    "        system_prompt = f\"\"\"Your name is HAL.\n",
    "You are an interactive CLI tool that helps with software engineering and production operations tasks.\n",
    "You are running on {str(os.uname())}, today is {datetime.now().strftime(\"%Y-%m-%d\")}\n",
    "\"\"\"\n",
    "\n",
    "        if toolbox.mcp_tools:\n",
    "            print(\n",
    "                f\"ðŸ”Œ Connected to {len(mcp_servers)} MCP server(s) with {len(toolbox.mcp_tools)} tool(s)\"\n",
    "            )\n",
    "\n",
    "        print(\"enter 'exit' to quit\")\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                user_input = input(\"> \")\n",
    "                if user_input.lower() == \"exit\":\n",
    "                    print(\"ðŸ‘‹ Goodbye!\")\n",
    "                    break\n",
    "                await loop(system_prompt, toolbox, messages, user_input)\n",
    "        except (KeyboardInterrupt, EOFError):\n",
    "            print(\"ðŸ‘‹ Goodbye!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter value for GITHUB_PERSONAL_ACCESS_TOKEN:  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Œ Connected to 1 MCP server(s) with 90 tool(s)\n",
      "enter 'exit' to quit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  Hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’­ The user is greeting me. I should respond in a friendly and helpful manner, introducing myself and offering assistance.\n",
      "ðŸ¤– Hello! I'm HAL, your interactive CLI assistant for software engineering and production operations tasks.\n",
      "\n",
      "I can help you with various GitHub-related tasks such as:\n",
      "- Managing repositories, issues, and pull requests\n",
      "- Searching code, issues, and repositories\n",
      "- Working with workflows and CI/CD\n",
      "- Reviewing notifications and managing your GitHub activity\n",
      "- Creating and updating files\n",
      "- And much more!\n",
      "\n",
      "How can I assist you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  what is my github username?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’­ The user is asking for their GitHub username. I can use the `get_me` tool to get details about the authenticated GitHub user.\n",
      "âœ… get_me:\n",
      "{\n",
      "  \"success\": true,\n",
      "  \"output\": \"{\\\"login\\\":\\\"jingkaihe\\\",\\\"id\\\":1335938,\\\"profile_url\\\":\\\"https://github.com/jingkaihe\\\",\\\"avatar_url\\\":\\\"https://avatars.githubusercontent.com/u/1335938?v=4\\\",\\\"details\\\":{\\\"name\\\":\\\"Jingkai He\\\",\\\"location\\\":\\\"London\\\",\\\"hireable\\\":true,\\\"public_repos\\\":68,\\\"public_gists\\\":29,\\\"followers\\\":38,\\\"following\\\":28,\\\"created_at\\\":\\\"2012-01-17T07:25:20Z\\\",\\\"updated_at\\\":\\\"2025-10-27T11:45:05Z\\\"}}\"\n",
      "}\n",
      "ðŸ¤– Your GitHub username is **jingkaihe**.\n",
      "\n",
      "Here are some of your profile details:\n",
      "- **Name:** Jingkai He\n",
      "- **Location:** London\n",
      "- **Public repositories:** 68\n",
      "- **Public gists:** 29\n",
      "- **Followers:** 38\n",
      "- **Following:** 28\n",
      "- **Profile:** https://github.com/jingkaihe\n",
      "\n",
      "Is there anything specific you'd like to do with your GitHub account?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‹ Goodbye!\n"
     ]
    }
   ],
   "source": [
    "mcp_servers = load_mcp_config(\"mcp.yaml\")\n",
    "\n",
    "await run_agent(\n",
    "    tools=[read_file, write_file, edit_file, shell], mcp_servers=mcp_servers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why You Probably Don't Need MCP\n",
    "\n",
    "While MCP offers elegant standardisation, that [mcp server proliferation](https://steipete.me/posts/2025/essential-reading-august-2025#less-is-more-the-hidden-costs-of-mcp-server-proliferation) creates a \"tragedy of the commons in your context window\". Everyone wants their tool included, but the cumulative effect makes the AI assistant worse, not better.\n",
    "\n",
    "### Context Window Economics\n",
    "\n",
    "Every MCP server and tool consumes precious tokens from your LLM's limited context window. Our GitHub example above exposed 90 tools to the agentâ€”that's 90 tool schemas taking up space that could be used for actual code, documentation, or reasoning. This creates a trade-off: adding more tools means less space for the work that actually matters.\n",
    "\n",
    "### The Allocation Paradox\n",
    "\n",
    "> \"Less is more. The more you allocate into the context window of an LLMâ€¦ the worse the outcomes you're going to get.\" â€” Geoffrey Huntley\n",
    "\n",
    "Each additional tool increases cognitive overhead for the LLM. With too many options, the model must spend reasoning capacity deciding *which* tool to use rather than focusing on solving your actual problem. This paradoxically makes the agent less effective, not more capable.\n",
    "\n",
    "### Tool Proliferation Problems\n",
    "\n",
    "Multiple similar tools create non-deterministic behavior. When you have overlapping capabilities (like Github MCP's `read_file` alongside the custom tool's `read_file`), the LLM struggles to choose between them. This confusion leads to:\n",
    "- Inconsistent tool selection across similar tasks\n",
    "- Wasted reasoning tokens evaluating redundant options\n",
    "- Unpredictable agent behavior that's hard to debug\n",
    "\n",
    "### Security Nightmares\n",
    "\n",
    "Third-party MCP servers introduce supply chain risks and potential attack vectors. Malicious actors could inject context that manipulates AI behavior, creating a new class of security vulnerabilities:\n",
    "- Unvetted code running with your agent's permissions\n",
    "- Potential for prompt injection through tool descriptions\n",
    "- Enterprise compliance issues with unaudited dependencies\n",
    "\n",
    "### A Better Approach\n",
    "\n",
    "**Start with custom tools first.** As this tutorial demonstrates, implementing tools is surprisingly straightforwardâ€”just a few lines of Python with the `@tool` decorator. Custom tools give you:\n",
    "- Full control over functionality and security\n",
    "- Minimal context window overhead (only what you need)\n",
    "- Easy debugging and maintenance\n",
    "- No external dependencies or supply chain risks\n",
    "\n",
    "**If you must use MCP:**\n",
    "- Limit servers to essential, irreplaceable functionality\n",
    "- Prefer first-party integrations from trusted vendors\n",
    "- Consider dynamically enabling/disabling servers based on specific workflow stages rather than loading everything at once\n",
    "- Regularly audit which tools are actually being used and remove the rest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
